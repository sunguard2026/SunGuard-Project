{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":41921,"sourceType":"datasetVersion","datasetId":28114}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# PIP installations\n# !pip freeze > packages.txt\n# !pip uninstall -y -r packages.txt\n!pip install \\\nnumpy==1.26.4 \\\nopencv-python==4.9.0.80 \\\npandas==2.2.2 \\\nmatplotlib==3.8.3 \\\ntqdm \\\nscipy==1.12.0 \\\nscikit-image==0.22.0 \\\nPyWavelets==1.5.0 \\\ntorch torchvision \\\nscikit-learn==1.2.1 \\\nxgboost \\\nlightgbm \\\ncatboost \\\nimbalanced-learn==0.12.4 \\\nPillow -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T18:13:07.074986Z","iopub.execute_input":"2025-11-27T18:13:07.075441Z","iopub.status.idle":"2025-11-27T18:13:10.569778Z","shell.execute_reply.started":"2025-11-27T18:13:07.075412Z","shell.execute_reply":"2025-11-27T18:13:10.569002Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!/bin/bash\n!curl -L -o /kaggle/working/synthesized_sdo.zip\\\n  https://www.kaggle.com/datasets/ef7b69fce738908b7ae67d0e4860d177203e61c5c55f4d3c19b5a3820c864e81","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T06:49:45.760015Z","iopub.execute_input":"2025-12-05T06:49:45.760636Z","iopub.status.idle":"2025-12-05T06:49:46.126022Z","shell.execute_reply.started":"2025-12-05T06:49:45.760599Z","shell.execute_reply":"2025-12-05T06:49:46.125320Z"}},"outputs":[{"name":"stdout","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  9088    0  9088    0     0  37635      0 --:--:-- --:--:-- --:--:-- 37709\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Imports\nimport os\nimport os.path as path\nimport json\nimport random\nfrom io import BytesIO\n\n# Use once at the beginning\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport warnings\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom tqdm import tqdm\nfrom scipy import ndimage\nfrom skimage.feature import canny, local_binary_pattern\nfrom skimage.filters import gabor\nimport pywt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\nfrom torch.optim.lr_scheduler import SequentialLR, ExponentialLR, CosineAnnealingLR\n\nimport torchvision.transforms as T\nfrom torchvision.io import read_image\n\nfrom torchvision.models import (\n    Swin_V2_B_Weights,\n    Swin_V2_S_Weights,\n    EfficientNet_V2_M_Weights,\n    EfficientNet_V2_S_Weights,\n    swin_v2_s,\n    swin_v2_b,\n    efficientnet_v2_m,\n    efficientnet_v2_s,\n)\nfrom torchvision.transforms import InterpolationMode\nfrom torchvision.ops import sigmoid_focal_loss\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    confusion_matrix,\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    matthews_corrcoef,\n    roc_auc_score,\n)\nfrom sklearn.ensemble import (\n    RandomForestClassifier,\n    ExtraTreesClassifier,\n    GradientBoostingClassifier,\n    HistGradientBoostingClassifier,\n    AdaBoostClassifier,\n    BaggingClassifier,\n)\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom imblearn.over_sampling import SMOTE\n\n# from transformers import DistilBertConfig, DistilBertForSequenceClassification\n# from transformers import RobertaConfig, RobertaForSequenceClassification\n\nwarnings.filterwarnings(\"ignore\")\nmatplotlib.use(\"Agg\")","metadata":{"jupyter":{"source_hidden":true},"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T18:13:10.570717Z","iopub.execute_input":"2025-11-27T18:13:10.570940Z","iopub.status.idle":"2025-11-27T18:13:16.239365Z","shell.execute_reply.started":"2025-11-27T18:13:10.570902Z","shell.execute_reply":"2025-11-27T18:13:16.238516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# metrics.py\ndef true_skill_statistic(y_true, y_pred):\n    x = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n    tn, fp, fn, tp = x\n    tpr = tp / (tp + fn)\n    fpr = fp / (fp + tn)\n    return float(tpr - fpr)\n\ndef far_score(y_true, y_pred):\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n    return fp / (tp + fp) if (tp + fp) > 0 else 0.0\n\ndef csi_score(y_true, y_pred):\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n    denom = tp + fp + fn\n    return tp / denom if denom > 0 else 0.0\n\ndef hss_score(y_true, y_pred):\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n    num = 2 * (tp * tn - fn * fp)\n    denom = ((tp + fn) * (fn + tn) + (tp + fp) * (fp + tn))\n    return num / denom if denom > 0 else 0.0","metadata":{"jupyter":{"source_hidden":true},"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T18:13:16.240287Z","iopub.execute_input":"2025-11-27T18:13:16.241002Z","iopub.status.idle":"2025-11-27T18:13:16.247649Z","shell.execute_reply.started":"2025-11-27T18:13:16.240973Z","shell.execute_reply":"2025-11-27T18:13:16.246836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# helpers.py\ndef process_meta_data(meta_data_path:str):\n    df = pd.read_csv(meta_data_path)\n    df = df.drop(columns=[\"start\", \"end\"])\n    df[\"peak_flux\"] = (df[\"peak_flux\"] > 1e-5).astype(int)\n    return df\n\nclass ReadImgs(nn.Module):\n    def __init__(self, form=\"stack\"):\n        super().__init__()\n        assert form == \"concat\" or form == \"stack\" or form == \"none\"\n        self.form = form\n    def forward(self, x):\n        x = x if isinstance(x, list) else [x]\n        imgs = []\n        for img_path in x:\n            try:\n                _img = read_image(img_path)[0]\n                imgs.append(_img)\n            except:\n                print(\"error reading image, path:\", img_path)\n\n        if self.form == \"concat\":\n            return torch.cat(imgs)\n        elif self.form == \"stack\":\n            return torch.stack(imgs)\n        elif self.form == \"none\":\n            return imgs\n        elif self.form == \"dict\":\n            return {img_path:img for img_path, img in zip(x, imgs)}\n\nclass GrayScaleToRGB(nn.Module):\n    def __init__(self):\n        super().__init__()\n    \n    def forward(self, x):\n        if x.ndim == 3:\n            return x.repeat(3, 1, 1)\n        \n        if x.ndim == 4:\n            return x.repeat(1, 3, 1, 1)\n\nclass FrequencyChannelTransform(nn.Module):\n    \"\"\"\n    Creates 3 channels: Original, FFT Magnitude, FFT Phase\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n    \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Tensor of shape (B, 1, H, W) or numpy array (H, W)\n        Returns:\n            Tensor of shape (B, 3, H, W)\n        \"\"\"\n        if isinstance(x, torch.Tensor):\n            is_batch = len(x.shape) == 4\n            if not is_batch:\n                x = x.unsqueeze(0)\n            \n            # Process each image in batch\n            batch_results = []\n            for img in x:\n                img_np = img.squeeze().cpu().numpy()\n                transformed = self._transform_single(img_np)\n                batch_results.append(transformed)\n            \n            result = torch.from_numpy(np.stack(batch_results, axis=0)).float()\n            return result if is_batch else result.squeeze(0)\n        else:\n            # Single numpy array\n            return torch.from_numpy(self._transform_single(x)).float()\n    \n    def _transform_single(self, gray_image):\n        # Original grayscale\n        original = gray_image\n        \n        # FFT\n        fft = np.fft.fft2(gray_image)\n        fft_shifted = np.fft.fftshift(fft)\n        \n        # Magnitude spectrum (log scale)\n        magnitude = np.log1p(np.abs(fft_shifted))\n        magnitude = (magnitude - magnitude.min()) / (magnitude.max() - magnitude.min() + 1e-8)\n        \n        # Phase spectrum\n        phase = np.angle(fft_shifted)\n        phase = (phase - phase.min()) / (phase.max() - phase.min() + 1e-8)\n        \n        return np.stack([original, magnitude, phase], axis=0)\n\nclass EdgeGradientChannelTransform(nn.Module):\n    \"\"\"\n    Creates 3 channels: Original, Edges, Gradient Magnitude\n    \"\"\"\n    def __init__(self, sigma=2.0):\n        super().__init__()\n        self.sigma = sigma\n    \n    def forward(self, x):\n        if isinstance(x, torch.Tensor):\n            is_batch = len(x.shape) == 4\n            if not is_batch:\n                x = x.unsqueeze(0)\n            \n            batch_results = []\n            for img in x:\n                img_np = img.squeeze().cpu().numpy()\n                transformed = self._transform_single(img_np)\n                batch_results.append(transformed)\n            \n            result = torch.from_numpy(np.stack(batch_results, axis=0)).float()\n            return result if is_batch else result.squeeze(0)\n        else:\n            return torch.from_numpy(self._transform_single(x)).float()\n    \n    def _transform_single(self, gray_image):\n        # Original\n        original = gray_image\n        \n        # Edge detection\n        edges = canny(gray_image, sigma=self.sigma).astype(np.float32)\n        \n        # Gradient magnitude\n        sobel_x = ndimage.sobel(gray_image, axis=0)\n        sobel_y = ndimage.sobel(gray_image, axis=1)\n        gradient = np.sqrt(sobel_x**2 + sobel_y**2)\n        gradient = (gradient - gradient.min()) / (gradient.max() - gradient.min() + 1e-8)\n        \n        return np.stack([original, edges, gradient], axis=0)\n\nclass WaveletChannelTransform(nn.Module):\n    \"\"\"\n    Creates 3 channels: Original, Wavelet Approximation, Wavelet Detail\n    \"\"\"\n    def __init__(self, wavelet='db4'):\n        super().__init__()\n        self.wavelet = wavelet\n    \n    def forward(self, x):\n        if isinstance(x, torch.Tensor):\n            is_batch = len(x.shape) == 4\n            if not is_batch:\n                x = x.unsqueeze(0)\n            \n            batch_results = []\n            for img in x:\n                img_np = img.squeeze().cpu().numpy()\n                transformed = self._transform_single(img_np)\n                batch_results.append(transformed)\n            \n            result = torch.from_numpy(np.stack(batch_results, axis=0)).float()\n            return result if is_batch else result.squeeze(0)\n        else:\n            return torch.from_numpy(self._transform_single(x)).float()\n    \n    def _transform_single(self, gray_image):\n        # Original\n        original = gray_image\n        \n        # Discrete Wavelet Transform\n        coeffs = pywt.dwt2(gray_image, self.wavelet)\n        cA, (cH, cV, cD) = coeffs\n        \n        # Approximation (low frequency)\n        approx = cv2.resize(cA, (gray_image.shape[1], gray_image.shape[0]))\n        approx = (approx - approx.min()) / (approx.max() - approx.min() + 1e-8)\n        \n        # Combined detail (high frequency)\n        detail = np.sqrt(cH**2 + cV**2 + cD**2)\n        detail = cv2.resize(detail, (gray_image.shape[1], gray_image.shape[0]))\n        detail = (detail - detail.min()) / (detail.max() - detail.min() + 1e-8)\n        \n        return np.stack([original, approx, detail], axis=0)\n\nclass TextureChannelTransform(nn.Module):\n    \"\"\"\n    Creates 3 channels: Original, LBP, Gabor\n    \"\"\"\n    def __init__(self, lbp_radius=1, lbp_points=8, gabor_frequency=0.2):\n        super().__init__()\n        self.lbp_radius = lbp_radius\n        self.lbp_points = lbp_points\n        self.gabor_frequency = gabor_frequency\n    \n    def forward(self, x):\n        if isinstance(x, torch.Tensor):\n            is_batch = len(x.shape) == 4\n            if not is_batch:\n                x = x.unsqueeze(0)\n            \n            batch_results = []\n            for img in x:\n                img_np = img.squeeze().cpu().numpy()\n                transformed = self._transform_single(img_np)\n                batch_results.append(transformed)\n            \n            result = torch.from_numpy(np.stack(batch_results, axis=0)).float()\n            return result if is_batch else result.squeeze(0)\n        else:\n            return torch.from_numpy(self._transform_single(x)).float()\n    \n    def _transform_single(self, gray_image):\n        # Original\n        original = gray_image\n        \n        # Local Binary Pattern\n        lbp = local_binary_pattern(gray_image, P=self.lbp_points, \n                                   R=self.lbp_radius, method='uniform')\n        lbp = (lbp - lbp.min()) / (lbp.max() - lbp.min() + 1e-8)\n        \n        # Gabor filter (texture)\n        gabor_real, _ = gabor(gray_image, frequency=self.gabor_frequency, theta=0)\n        gabor_feat = (gabor_real - gabor_real.min()) / (gabor_real.max() - gabor_real.min() + 1e-8)\n        \n        return np.stack([original, lbp, gabor_feat], axis=0)\n\nclass ComprehensiveChannelTransform(nn.Module):\n    \"\"\"\n    Creates 5 channels: Original, FFT Magnitude, Edges, Gradient, Wavelet Detail\n    \"\"\"\n    def __init__(self, sigma=2.0, wavelet='db4'):\n        super().__init__()\n        self.sigma = sigma\n        self.wavelet = wavelet\n    \n    def forward(self, x):\n        if isinstance(x, torch.Tensor):\n            is_batch = len(x.shape) == 4\n            if not is_batch:\n                x = x.unsqueeze(0)\n            \n            batch_results = []\n            for img in x:\n                img_np = img.squeeze().cpu().numpy()\n                transformed = self._transform_single(img_np)\n                batch_results.append(transformed)\n            \n            result = torch.from_numpy(np.stack(batch_results, axis=0)).float()\n            return result if is_batch else result.squeeze(0)\n        else:\n            return torch.from_numpy(self._transform_single(x)).float()\n    \n    def _transform_single(self, gray_image):\n        original = gray_image\n        \n        # FFT Magnitude\n        fft = np.fft.fft2(gray_image)\n        fft_mag = np.log1p(np.abs(np.fft.fftshift(fft)))\n        fft_mag = (fft_mag - fft_mag.min()) / (fft_mag.max() - fft_mag.min() + 1e-8)\n        \n        # Edges\n        edges = canny(gray_image, sigma=self.sigma).astype(np.float32)\n        \n        # Gradient\n        sobel_x = ndimage.sobel(gray_image, axis=0)\n        sobel_y = ndimage.sobel(gray_image, axis=1)\n        gradient = np.sqrt(sobel_x**2 + sobel_y**2)\n        gradient = (gradient - gradient.min()) / (gradient.max() - gradient.min() + 1e-8)\n        \n        # Wavelet\n        coeffs = pywt.dwt2(gray_image, self.wavelet)\n        cA, (cH, cV, cD) = coeffs\n        detail = np.sqrt(cH**2 + cV**2 + cD**2)\n        detail = cv2.resize(detail, (gray_image.shape[1], gray_image.shape[0]))\n        detail = (detail - detail.min()) / (detail.max() - detail.min() + 1e-8)\n        \n        return np.stack([original, fft_mag, edges, gradient, detail], axis=0)\n\nclass RepeatChannelTransform(nn.Module):\n    \"\"\"\n    Baseline: Simply repeats grayscale channel 3 times\n    \"\"\"\n    def __init__(self, num_repeats=3):\n        super().__init__()\n        self.num_repeats = num_repeats\n    \n    def forward(self, x):\n        if isinstance(x, torch.Tensor):\n            if len(x.shape) == 2:  # (H, W)\n                return x.unsqueeze(0).repeat(self.num_repeats, 1, 1)\n            elif len(x.shape) == 3:  # (1, H, W)\n                return x.repeat(self.num_repeats, 1, 1)\n            elif len(x.shape) == 4:  # (B, 1, H, W)\n                return x.repeat(1, self.num_repeats, 1, 1)\n        else:\n            return torch.from_numpy(np.stack([x] * self.num_repeats, axis=0)).float()\n\nclass SDOCacheTransform(nn.Module):\n    def __init__(self, chunks=False):\n        super().__init__()\n        self.stacking_dim = int(chunks)\n        pass\n    def forward(self, x):\n        lst = []\n        for wavelength in x.keys():\n            lst.append(x[wavelength])\n        return torch.stack(lst, dim=self.stacking_dim)\n\nclass RandomRotate90(nn.Module):\n    def __init__(self, angles=(0, 90, 180, 270)):\n        super().__init__()\n        self.angles = angles\n\n    def __call__(self, img):\n        # img can be PIL Image or Tensor\n        angle = random.choice(self.angles)\n        return T.functional.rotate(img, angle)\n\n    def str(self):\n        return f\"RandomRotate90(angles={self.angles})\"\n    def __repr__(self):\n        return str(self)\n\ndef write_json(dict, dir):\n    with open(dir, \"w\", encoding=\"utf-8\") as f:\n        f.write(json.dumps(dict))\n\ndef write_note(txt, dir):\n    with open(dir, \"w\", encoding=\"utf-8\") as f:\n        f.write(txt)\n\ndef read_json(dir):\n    with open(dir, \"r\", encoding=\"utf-8\") as f:\n            return json.loads(f.read())\n\ndef list_models():\n    weights = read_json(\"weights.json\")\n    weights = sorted(weights, key=lambda x:-float(x[\"acc\"]))\n    weights = list(filter(lambda x : \"IMAGENET1K_V1\" in x[\"weight\"], weights))\n    for model in weights:\n        print(\"acc:\", model[\"acc\"], \"||\", \"# of parameters\", model[\"params\"], \"||\", \"weight:\", model[\"weight\"])\n\ndef create_dummy_data(wavelengths, n=32):\n    data_gen = lambda : {wavelength:torch.randn(1280) for wavelength in wavelengths}\n    label_gen = lambda : torch.randint(0, 1, size=(1,), dtype=torch.float32)\n    return [(data_gen(), label_gen()) for _ in range(n)]\n\ndef print_run_summary(dict, threshold):\n    print(f\"\\n[Threshold: {threshold}]\")\n    print(f\"{'Name':<15} | {'Score':>8}\")\n    print(\"-\" * 26)\n    for name, score in dict.items():\n        print(f\"{name:<15} | {score:>8.4f}\")\n\ndef summarize_performance_table(performances, width=15):\n    titles = [col_name.center(width) for col_name in performances.keys()]\n\n    s = (\"  \" + \"_\" * (width)) * (1 + len(titles)) + \"  \"\n    print(s)\n    print(\"||\" + \"||\".join([\"metric\".center(width), *titles]) + \"||\")\n    print(s.replace(\"  \", \"||\"))\n\n    for metric in performances[max(performances, key=lambda x: len(performances[x].keys()))].keys():\n        lst = []\n        for key in performances.keys():\n            score = performances[key].get(metric, \"Nan\")\n            score = round(score, 4) if isinstance(score, float) else score\n            score = str(score)\n            lst.append(score)\n        print(\"||\" + \"||\".join([metric.center(width), *[x.center(width) for x in lst]]) + \"||\")\n        print(s.replace(\"  \", \"||\"))\n\ndef evaluate(y_true, y_pred, metrics, threshold=0.5):\n    scores = {}\n    \n    if isinstance(threshold, str):\n        optimial_threshold = 0\n        max_score = 0\n        for _threshold in torch.linspace(start=0, end=1, steps=100).tolist():\n            score = metrics[threshold](y_true, y_pred > _threshold)\n            if score > max_score:\n                max_score = score\n                optimial_threshold = _threshold\n        threshold = optimial_threshold\n    \n    for name, metric in metrics.items():\n        scores[name] = metric(y_true, y_pred > threshold)\n        \n    return scores, threshold\n\ndef plot_frequency_bars(columns, labels, show_img=False):\n    \"\"\"\n    Takes any number of pandas Series/columns and creates bar plots showing \n    the frequency of 0s and 1s in each column.\n    \n    Args:\n        *columns: Variable number of pandas Series/columns to plot\n        \n    Returns:\n        tuple: (img, *counts)\n            - img: PIL Image object with the bar plots\n            - counts: Series with value counts for each column\n    \"\"\"\n    if not columns:\n        raise ValueError(\"At least one column must be provided\")\n    \n    # Calculate value counts for each column\n    counts = []\n    for col in columns:\n        count = col.value_counts().reindex([0, 1], fill_value=0)\n        counts.append(count)\n    \n    # Determine subplot layout\n    n_plots = len(columns)\n    n_cols = min(3, n_plots)  # Max 3 columns\n    n_rows = (n_plots + n_cols - 1) // n_cols\n    \n    # Create figure with subplots\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 5*n_rows))\n    fig.suptitle('Peak flux distribution', fontsize=14, fontweight='bold')\n    \n    # Flatten axes array for easier iteration\n    if n_plots == 1:\n        axes = [axes]\n    else:\n        axes = axes.flatten() if n_plots > 1 else [axes]\n    \n    # Find global max for consistent y-axis\n    global_max = max(count.max() for count in counts)\n    \n    # Plot each column\n    for idx, (col, count) in enumerate(zip(columns, counts)):\n        ax = axes[idx]\n        \n        # Create bar plot\n        ax.bar([0, 1], count.values, color=['#e74c3c', '#2ecc71'], \n               alpha=0.8, edgecolor='black', width=0.6)\n        ax.set_xticks([0, 1])\n        ax.set_xticklabels(['0', '1'])\n        ax.set_title(col.name or f'Column {idx+1}', fontsize=12, fontweight='bold')\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        ax.grid(axis='y', alpha=0.3)\n        ax.set_ylim(0, global_max * 1.15)\n        \n        # Add value labels on bars with percentages\n        for i, (v, label) in enumerate(zip(count.values, labels)):\n            ax.text(i, v, label, ha='center', va='bottom', \n                   fontsize=10, fontweight='bold')\n    \n    # Hide unused subplots\n    for idx in range(n_plots, len(axes)):\n        axes[idx].set_visible(False)\n    \n    plt.tight_layout()\n    \n    # Convert plot to image\n    buf = BytesIO()\n    plt.savefig(buf, format='png', dpi=100, bbox_inches='tight')\n    buf.seek(0)\n    img = Image.open(buf)\n    plt.close(fig)\n    \n    counts = [(int(count[0]), int(count[1])) for count in counts]\n    \n    if show_img:\n        display(img)\n    \n    return (img, *counts)\n\ndef visualize_dict_list(data_list, save_dir=None, show_summary=True, title='Data Visualization', exclude=[]):\n    \"\"\"\n    Takes a list of dictionaries and creates a multi-subplot visualization.\n    Each key gets its own subplot with values plotted over the list indices.\n    \n    Args:\n        data_list: List of dictionaries with numeric values\n        \n    Returns:\n        PIL Image object containing the visualization\n    \"\"\"\n    if not data_list:\n        raise ValueError(\"data_list cannot be empty\")\n    \n    # Get all unique keys from all dictionaries\n    all_keys = set()\n    for d in data_list:\n        all_keys.update(d.keys())\n    all_keys = sorted(all_keys)\n    \n    for key in exclude:\n        all_keys.remove(key)\n    \n    # Determine subplot layout\n    n_plots = len(all_keys)\n    n_cols = min(3, n_plots)  # Max 3 columns\n    n_rows = (n_plots + n_cols - 1) // n_cols\n    \n    # Create figure with subplots\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n    fig.suptitle(title, fontsize=16, fontweight='bold')\n    \n    # Flatten axes array for easier iteration\n    if n_plots == 1:\n        axes = [axes]\n    else:\n        axes = axes.flatten() if n_plots > 1 else [axes]\n    \n    # Plot each key\n    for idx, key in enumerate(all_keys):\n        ax = axes[idx]\n        \n        # Extract values for this key\n        values = [d.get(key) for d in data_list]\n        indices = list(range(1, len(data_list) + 1))\n        \n        # Filter out None values for plotting\n        valid_pairs = [(i, v) for i, v in zip(indices, values) if v is not None]\n        if valid_pairs:\n            valid_indices, valid_values = zip(*valid_pairs)\n            ax.plot(valid_indices, valid_values, marker='o', linewidth=2, markersize=6)\n            \n            # Calculate min and max\n            min_val = min(valid_values)\n            max_val = max(valid_values)\n            title = f\"{key}\\nMin: {min_val:.2f} | Max: {max_val:.2f}\"\n        else:\n            title = f\"{key}\\n(No data)\"\n        \n        ax.set_title(title, fontsize=12, fontweight='bold')\n        ax.set_xlabel('Epoch')\n        ax.set_ylabel('Scores')\n        ax.grid(True, alpha=0.3)\n    \n    # Hide unused subplots\n    for idx in range(n_plots, len(axes)):\n        axes[idx].set_visible(False)\n    \n    plt.tight_layout()\n    \n    # Convert plot to image\n    buf = BytesIO()\n    plt.savefig(buf, format='png', dpi=100, bbox_inches='tight')\n    buf.seek(0)\n    img = Image.open(buf)\n    plt.close(fig)\n    \n    if show_summary:\n        display(img)\n    if save_dir:\n        img.save(save_dir)\n        print(\"saving images\")\n\n    return img\n\ndef plot_metric_bars(results, names, ignore_keys=(\"epoch\", \"threshold\", \"threhsold\"),\n                    cols=4, save_dir=None, metric=\"accuracy\"):\n    if not results:\n        raise ValueError(\"Empty results list\")\n\n    # metrics to plot\n    metrics = [k for k in results[0].keys() if k not in ignore_keys]\n    n = len(metrics)\n    rows = (n + cols - 1) // cols\n\n    # --- SORT MODELS BY FIRST METRIC IN DESCENDING ORDER ---\n    primary_metric = metric\n    order = np.argsort([-r[primary_metric] for r in results])  # descending\n\n    results = [results[i] for i in order]\n    names   = [names[i]   for i in order]\n\n    x = np.arange(len(results))\n\n    fig, axes = plt.subplots(rows, cols, figsize=(8 * cols, 3.5 * rows))\n    axes = np.array(axes).reshape(-1)\n\n    for idx, metric in enumerate(metrics):\n        ax = axes[idx]\n        values = [r[metric] for r in results]\n\n        ax.bar(x, values)\n        ax.set_title(f\"{metric} (sorted by {primary_metric})\")\n        ax.set_xticks(x)\n        ax.set_xticklabels(names, rotation=45, ha=\"right\")\n        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n\n    # hide empty axes\n    for j in range(len(metrics), len(axes)):\n        axes[j].axis(\"off\")\n\n    plt.tight_layout()\n\n    if save_dir:\n        plt.savefig(save_dir, dpi=300)\n\n    buf = BytesIO()\n    plt.savefig(buf, format='png', dpi=100, bbox_inches='tight')\n    img = Image.open(buf)\n\n    display(img)\n\ndef check_incomplete(wavelengths_dir, num_epochs):\n    incomlete = filter(\n        lambda wavelength : read_json(path.join(wavelengths_dir, wavelength, \"config.json\"))[\"training_epoch\"] < (num_epochs - 1),\n        [d for d in os.listdir(wavelengths_dir) if path.exists(path.join(wavelengths_dir, d, \"config.json\"))]\n        )\n    incomlete = list(incomlete)\n    return [incomlete[0]] if len(incomlete) > 0 else []\n\ndef set_global_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    # For full reproducibility (slower)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef pick_best_model(wavelength_dir, metric, dynamic=True):\n    p = path.join(wavelength_dir, \"val_records\", \"dynamic\" if dynamic else \"fixed\")\n    records = [read_json(path.join(p, record)) for record in os.listdir(p)]\n    best_record = max(records, key=lambda x: x[metric])\n    return f\"ckpt_{best_record['epoch'] * 2}.pt\", best_record","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T18:13:16.249496Z","iopub.execute_input":"2025-11-27T18:13:16.249693Z","iopub.status.idle":"2025-11-27T18:13:16.309088Z","shell.execute_reply.started":"2025-11-27T18:13:16.249676Z","shell.execute_reply":"2025-11-27T18:13:16.308274Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# dataset.py\nclass WaveLenghtDataset(Dataset):\n    def __init__(self, base_route:str, meta_labels:pd.DataFrame, wavelengths, transform=None, augmentation=None, augmentation_class=\"both\"):\n\n        assert augmentation_class in [\"positive\", \"negative\", \"both\"]\n\n        self._metadata = meta_labels\n        self._wavelengths = wavelengths\n        self._transform = transform\n        self._augmentation = augmentation\n        self.augmentation_class = augmentation_class\n\n        self._paths = {\n            (fileid + \"_\" + filename) : path.join(base_route, fileid, filename, image)\n            for fileid in os.listdir(base_route) if not fileid.endswith(\".csv\")\n            for filename in os.listdir(path.join(base_route, fileid))\n            for image in os.listdir(path.join(base_route, fileid, filename))\n            if image.split(\"__\")[-1].split(\".\")[0] == self._wavelengths\n        }\n\n    def __len__(self):\n        return len(self._metadata)\n\n    def get_labels(self):\n        return self._metadata[\"peak_flux\"]\n\n    def __getitem__(self, idx):\n        try:\n            image_id, label = self._metadata.iloc[idx]\n            image = self._paths[image_id]\n\n            if self._transform:\n                image = self._transform(image)\n\n            if self.augmentation_class in [\"positive\", \"both\"] and label == 1:\n                if self._augmentation:\n                    image = self._augmentation(image)\n\n            if self.augmentation_class in [\"negative\", \"both\"] and label == 0:\n                if self._augmentation:\n                    image = self._augmentation(image)\n                \n            label = torch.tensor(label, dtype=torch.float32)\n            return image, label\n        except:\n            return self[(idx+1) % len(self)]\n    \n    def set_transform(self, t):\n        self._transform = t\n\nclass WaveLenghtDatasetV2(Dataset):\n    def __init__(self, base_route:str, meta_labels:pd.DataFrame, wavelengths, transform=None, augmentation=None, augmentation_class=\"both\"):\n\n        assert augmentation_class in [\"positive\", \"negative\", \"both\"]\n\n        self._metadata = meta_labels\n        self._wavelengths = wavelengths\n        self._transform = transform\n        self._augmentation = augmentation\n        self.augmentation_class = augmentation_class\n        self._paths = {\n            (fileid + \"_\" + filename) :[\n                path.join(base_route, fileid, filename, image)\n                for image in filter(\n                        lambda image : image.split(\"__\")[-1].split(\".\")[0] == self._wavelengths,\n                        os.listdir(path.join(base_route, fileid, filename))\n                        )\n                ]\n            for fileid in os.listdir(base_route) if not fileid.endswith(\".csv\")\n            for filename in os.listdir(path.join(base_route, fileid))\n        }\n\n        self._indices = [(i, j) for i, _id in enumerate(self._metadata[\"id\"]) for j in range(len(self._paths[_id]))]\n\n    def __len__(self):\n        return len(self._indices)\n\n    def __getitem__(self, idx):\n        try:\n            file_idx, image_idx = self._indices[idx]\n            image_id, label = self._metadata.iloc[file_idx]\n            image = self._paths[image_id][image_idx]\n\n            if self._transform:\n                image = self._transform(image)\n\n            if self.augmentation_class in [\"positive\", \"both\"] and label == 1:\n                if self._augmentation:\n                    image = self._augmentation(image)\n\n            if self.augmentation_class in [\"negative\", \"both\"] and label == 0:\n                if self._augmentation:\n                    image = self._augmentation(image)\n                \n            label = torch.tensor(label, dtype=torch.float32)\n            return image, label\n        except:\n            return self[(idx+1) % len(self)]\n\n    def set_transform(self, t):\n        self._transform = t\n\nclass SDODataset(Dataset):\n    def __init__(self, base_route:str, metadata:pd.DataFrame, wavelengths, transform=None, augmentation=None, augmentation_class=\"both\"):\n        assert augmentation_class in [\"positive\", \"negative\", \"both\"]\n\n        self._wavelengths = wavelengths\n        self._transform = transform\n        self._augmentation = augmentation\n        self.augmentation_class = augmentation_class\n\n        self._paths = {\n            (fileid + \"_\" + filename) : path.join(base_route, fileid, filename)\n            for fileid in os.listdir(base_route) if not fileid.endswith(\".csv\")\n            for filename in os.listdir(path.join(base_route, fileid))\n            if len(os.listdir(path.join(base_route, fileid, filename))) == 40\n        }\n\n        self._metadata = metadata[metadata[\"id\"].isin(list(self._paths.keys()))]\n\n    def __len__(self):\n        return len(self._metadata)\n\n    def __getitem__(self, idx):\n            image_id, label = self._metadata.iloc[idx]\n            images_dir = self._paths[image_id]\n            images = [path.join(images_dir, image) for image in os.listdir(images_dir)]\n            images_dict = {\n                wavelength : list(filter(lambda img_dir : wavelength == img_dir.split(\"__\")[1].split(\".\")[0], images))\n                for wavelength in self._wavelengths\n            }\n\n            for wavelength in self._wavelengths:\n                images = images_dict[wavelength]\n                if self._transform:\n                    images = self._transform(images)\n\n                if self.augmentation_class in [\"positive\", \"both\"] and label == 1:\n                    if self._augmentation:\n                        images = self._augmentation(images)\n\n                if self.augmentation_class in [\"negative\", \"both\"] and label == 0:\n                    if self._augmentation:\n                        images = self._augmentation(images)\n\n                images_dict[wavelength] = images\n\n            label = torch.tensor(label, dtype=torch.float32)\n            return images_dict, label\n    \n    def set_transform(self, t):\n        self._transform = t\n\n    # def collate_fn(self, lst):\n    #     # d = \n    #     pass\n\nclass CacheDataset(Dataset):\n    def __init__(self, cache_dir, transforms=None, augmentation=None, load_device=None):\n        self.load_device = load_device\n        self.base_dir = cache_dir\n        self._transform = transforms\n        self._augmentation = augmentation\n        self.ordered_dirs = sorted([\n            path.join(label, idx)\n            for label in os.listdir(self.base_dir)\n            for idx in os.listdir(path.join(self.base_dir, label))\n            ], key=lambda x: int(x.split(\"/\")[-1].split(\"_\")[0]))\n    def __getitem__(self, idx):\n        _dir = path.join(self.base_dir, self.ordered_dirs[idx])\n        data = torch.load(_dir, weights_only=False, map_location=self.load_device)\n\n        if self._transform:\n            data = self._transform(data)\n        if self._augmentation:\n            data = self._augmentation(data)\n\n        label = torch.tensor(int(self.ordered_dirs[idx].split(\"/\")[0]), dtype=torch.float32)\n\n        return data, label\n\n    def get_labels(self):\n        return [int(_dir.split(\"/\")[0]) for _dir in self.ordered_dirs]\n\n    def __len__(self):\n        return len(self.ordered_dirs)\n\nclass ListDataset(Dataset):\n    def __init__(self, list, transforms=None, augmentation=None, load_device=None):\n        self.load_device = load_device\n        self._transform = transforms\n        self._augmentation = augmentation\n        self.list = list\n    def __getitem__(self, idx):\n        data, label = self.list[idx]\n        if self._transform:\n            data = self._transform(data)\n        if self._augmentation:\n            data = self._augmentation(data)\n        return data, label\n\n    def __len__(self):\n        return len(self.list)\n\nclass SynthesizedDataset(Dataset):\n    def __init__(self, base_dir, wavelength, synthesized_class, transform=None, augmentation=None, num_samples=\"all\"):\n        self.base_dir = base_dir\n        self.wavelength = wavelength\n        self._transform = transform\n        self._augmentation = augmentation\n        self.synthesized_class = synthesized_class\n\n        imgs_filename = os.listdir(path.join(base_dir, wavelength))\n\n        if num_samples < 1 and num_samples > 0:\n            num_samples = int(num_samples * len(imgs_filename))\n        elif num_samples == \"all\":\n            num_samples = len(imgs_filename)\n\n        self.imgs_filename = imgs_filename[:num_samples]\n\n    def __getitem__(self, idx):\n        data = path.join(self.base_dir, self.wavelength, self.imgs_filename[idx])\n\n        if self._transform:\n            data = self._transform(data)\n        if self._augmentation:\n            data = self._augmentation(data)\n        return data, torch.tensor(self.synthesized_class, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.imgs_filename)\n\nclass MergedDatasets(Dataset):\n    def __init__(self, dataset, *datasets):\n        self.datasets = [dataset, *datasets]\n        starts = torch.cumsum(torch.tensor([len(dataset) for dataset in self.datasets]), dim=0).tolist()\n        self._starts = [0, *starts[:-1]]\n        self._transform = [dataset._transform for dataset in self.datasets]\n        self._augmentation = [dataset._augmentation for dataset in self.datasets]\n\n    def index_router(self, idx):\n        d_idx = 0\n        for d in self.datasets:\n            if idx < len(d):\n                break\n            d_idx += 1\n            idx -= len(d)\n        return d_idx, idx\n\n    def __getitem__(self, idx):\n        d_idx, idx = self.index_router(idx)\n        return self.datasets[d_idx][idx]\n    \n    def __len__(self):\n        return sum([len(d) for d in self.datasets])\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T18:13:16.309938Z","iopub.execute_input":"2025-11-27T18:13:16.310210Z","iopub.status.idle":"2025-11-27T18:13:16.338847Z","shell.execute_reply.started":"2025-11-27T18:13:16.310182Z","shell.execute_reply":"2025-11-27T18:13:16.337990Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model.py\nclass Conv1DClassifier(nn.Module):\n    def __init__(self, in_channel, num_classes=2):\n        super().__init__()\n        \n        # Stem\n        self.stem = nn.Sequential(\n            nn.Conv1d(in_channel, 256, kernel_size=7, stride=2, padding=3),\n            nn.BatchNorm1d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n        )\n        \n        # Convolutional blocks (similar to EfficientNet-ish expansion)\n        self.blocks = nn.Sequential(\n            nn.Conv1d(256, 512, kernel_size=3, padding=1),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            \n            nn.Conv1d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            \n            nn.Conv1d(512, 1024, kernel_size=3, padding=1),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(inplace=True),\n            \n            nn.Conv1d(1024, 1024, kernel_size=3, padding=1),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(inplace=True),\n        )\n        \n        # Pool & classifier\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        self.classifier = nn.Linear(1024, num_classes)\n        \n    def forward(self, x):\n        x = self.stem(x)\n        x = self.blocks(x)\n        x = self.pool(x).squeeze(-1)\n        x = self.classifier(x)\n        return x\n\n    @property\n    def num_parameters(self):\n        return sum(p.numel() for p in self.parameters())\n\nclass ResidualBlock1D(nn.Module):\n    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1):\n        super().__init__()\n        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size, stride=stride, padding=kernel_size//2, bias=False)\n        self.bn1 = nn.BatchNorm1d(out_ch)\n        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size, stride=1, padding=kernel_size//2, bias=False)\n        self.bn2 = nn.BatchNorm1d(out_ch)\n        self.skip = nn.Conv1d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n        \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.skip(x)\n        return F.relu(out)\n\nclass WideResNet1D(nn.Module):\n    def __init__(self, in_channels=10, num_channels=[4096, 4096, 1024], num_classes=1):\n        super().__init__()\n        self.in_block = ResidualBlock1D(in_channels, num_channels[0])\n\n        blocks = []\n        for i in range(1, len(num_channels)):\n            blocks.append(ResidualBlock1D(num_channels[i-1], num_channels[i]))\n\n        self.blocks = nn.ModuleList(blocks)\n        self.global_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc = nn.Linear(num_channels[-1], num_classes)\n        \n    def forward(self, x):\n        x = self.in_block(x)\n        for block in self.blocks:\n            x = block(x)\n        x = self.global_pool(x).squeeze(-1)\n        x = self.fc(x)\n        return x\n\nclass Classifier:\n    def __init__(self, model_name=\"random_forest\", use_smote=True, verbose=0, random_state=42):\n        self.use_smote = use_smote\n        self.verbose = verbose\n        self.random_state = random_state\n\n        # Instantiate model based on name with larger configurations\n        if model_name == \"random_forest\":\n            self.model = RandomForestClassifier(\n                n_estimators=500,\n                max_depth=30,\n                min_samples_split=2,\n                min_samples_leaf=1,\n                max_features='sqrt',\n                n_jobs=-1,\n                random_state=self.random_state\n            )\n        elif model_name == \"extra_trees\":\n            self.model = ExtraTreesClassifier(\n                n_estimators=500,\n                max_depth=30,\n                min_samples_split=2,\n                min_samples_leaf=1,\n                max_features='sqrt',\n                n_jobs=-1,\n                random_state=self.random_state\n            )\n        elif model_name == \"gradient_boosting\":\n            self.model = GradientBoostingClassifier(\n                n_estimators=500,\n                learning_rate=0.05,\n                max_depth=7,\n                min_samples_split=2,\n                min_samples_leaf=1,\n                subsample=0.8,\n                random_state=self.random_state\n            )\n        elif model_name == \"hist_gradient_boosting\":\n            self.model = HistGradientBoostingClassifier(\n                max_iter=500,\n                learning_rate=0.05,\n                max_depth=15,\n                min_samples_leaf=10,\n                random_state=self.random_state,\n                verbose=self.verbose\n            )\n        elif model_name == \"ada_boost\":\n            self.model = AdaBoostClassifier(\n                n_estimators=500,\n                learning_rate=0.5,\n                random_state=self.random_state\n            )\n        elif model_name == \"bagging\":\n            self.model = BaggingClassifier(\n                n_estimators=500,\n                max_samples=0.8,\n                max_features=0.8,\n                n_jobs=-1,\n                random_state=self.random_state\n            )\n        elif model_name == \"xgboost\":\n            self.model = XGBClassifier(\n                n_estimators=500,\n                learning_rate=0.05,\n                max_depth=10,\n                min_child_weight=1,\n                subsample=0.8,\n                colsample_bytree=0.8,\n                gamma=0,\n                reg_alpha=0.1,\n                reg_lambda=1,\n                use_label_encoder=False,\n                eval_metric='logloss',\n                tree_method='hist',\n                n_jobs=-1,\n                random_state=self.random_state,\n                verbosity=self.verbose\n            )\n        elif model_name == \"linear_svm\":\n            self.model = SVC(\n                kernel='linear',\n                C=1.0,\n                probability=True,\n                cache_size=1000,\n                random_state=self.random_state,\n                verbose=(self.verbose>0)\n            )\n        elif model_name == \"lightgbm\":\n            self.model = LGBMClassifier(\n                n_estimators=500,\n                learning_rate=0.05,\n                max_depth=15,\n                num_leaves=63,\n                min_child_samples=10,\n                subsample=0.8,\n                colsample_bytree=0.8,\n                reg_alpha=0.1,\n                reg_lambda=1,\n                n_jobs=-1,\n                random_state=self.random_state,\n                verbose=self.verbose\n            )\n        elif model_name == \"catboost\":\n            self.model = CatBoostClassifier(\n                iterations=500,\n                learning_rate=0.05,\n                depth=10,\n                l2_leaf_reg=3,\n                subsample=0.8,\n                random_strength=1,\n                thread_count=-1,\n                verbose=self.verbose,\n                random_state=self.random_state\n            )\n        else:\n            raise ValueError(f\"Unknown model name: {model_name}\")\n    \n    def fit(self, X, y, X_val=None, y_val=None):\n        \"\"\"Fit model. Optionally apply SMOTE for imbalance. For boosting, supports eval set for monitoring.\"\"\"\n        \n        X = X.numpy()\n        y = y.numpy()\n        \n        if X_val is not None:\n            X_val = X_val.numpy()\n        if y_val is not None:\n            y_val = y_val.numpy()\n\n        if self.use_smote:\n            smote = SMOTE(random_state=self.random_state)\n            X_res, y_res = smote.fit_resample(X, y)\n        else:\n            X_res, y_res = X, y\n\n        # Fit model\n        if isinstance(self.model, (XGBClassifier, LGBMClassifier, CatBoostClassifier)) and X_val is not None and y_val is not None:\n            self.model.fit(X_res, y_res, eval_set=[(X_val, y_val)], verbose=self.verbose)\n        else:\n            self.model.fit(X_res, y_res)\n    \n    def __call__(self, X, return_prob=True):\n        X = X.detach().numpy()\n        \"\"\"Make predictions. Return probabilities if requested.\"\"\"\n        if return_prob:\n            if hasattr(self.model, \"predict_proba\"):\n                x = self.model.predict_proba(X)[:, 1]\n                return torch.from_numpy(x)\n            else:\n                raise ValueError(\"Model does not support predict_proba\")\n        else:\n            x = self.model.predict(X)\n            return torch.from_numpy(x)\n    \n    def save(self, path):\n        \"\"\"Save the fitted model to disk\"\"\"\n        with open(path, \"wb\") as f:\n            pickle.dump(self.model, f)\n\n    def load(self, path):\n        with open(path, \"rb\") as f:\n            self.model = pickle.load(f)\n\n    @staticmethod\n    def list_models():\n        return [\"random_forest\", \"extra_trees\", \"gradient_boosting\", \"hist_gradient_boosting\", \n                \"ada_boost\", \"bagging\", \"xgboost\", \"linear_svm\", \"lightgbm\", \"catboost\"]","metadata":{"jupyter":{"source_hidden":true},"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T18:13:16.339705Z","iopub.execute_input":"2025-11-27T18:13:16.339942Z","iopub.status.idle":"2025-11-27T18:13:16.363514Z","shell.execute_reply.started":"2025-11-27T18:13:16.339908Z","shell.execute_reply":"2025-11-27T18:13:16.362671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# losses.py\nclass HybridLossFunction(nn.Module):\n    def __init__(self, function, *functions, weights=None):\n        super().__init__()\n        functions = [function, *functions]\n        if weights is None:\n            weights = [1/len(functions) for _ in range(len(functions))]\n        assert len(functions) == len(weights)\n        self.functions = functions\n        self.weights = weights\n\n    def forward(self, predictions, label):\n        total_loss = 0  # ADD THIS\n        for function, weight in zip(self.functions, self.weights):\n            total_loss += function(predictions, label) * weight  # ACCUMULATE\n        return total_loss  # RETURN TOTAL\n    \n\n    def __str__(self):\n        fns = \",\\n\".join([str(fn) + \" * \" + str(w) for w, fn in zip(self.weights, self.functions)])\n        return f'HybridLossFunction(\\n{fns}\\n)'\n    def __repr__(self):\n        return str(self)\n\nclass InverseFreqWeightedBCE(nn.Module):\n    def __init__(self, weights, *args, **kwargs):\n        super().__init__()\n\n        self.class_weights = torch.tensor(weights, dtype=torch.float32, requires_grad=False)\n\n        self.bce = nn.BCEWithLogitsLoss(*args, **kwargs, pos_weight=self.class_weights[1] / self.class_weights[0])\n\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n        return self.bce(logits, targets)\n\n    def __str__(self):\n        return f'InverseFreqWeightedBCE(weights={str(self.class_weights.tolist())},{str(self.bce)[str(self.bce).index(\"()\"):]}'\n    def __repr__(self):\n        return str(self)\n\nclass AsymmetricLoss(nn.Module):\n    def __init__(self, gamma_pos=1.0, gamma_neg=4.0, eps=1e-8):\n        super().__init__()\n        self.gamma_pos = gamma_pos\n        self.gamma_neg = gamma_neg\n        self.eps = eps\n\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n        probs = torch.sigmoid(logits).clamp(self.eps, 1 - self.eps)\n\n        pos_loss = targets * (1 - probs) ** self.gamma_pos * torch.log(probs)\n        neg_loss = (1 - targets) * probs ** self.gamma_neg * torch.log(1 - probs)\n\n        loss = - (pos_loss + neg_loss)\n        return loss.mean()\n\n    def __str__(self):\n        return f\"AsymmetricLoss(gamma_pos={self.gamma_pos}, gamma_neg={self.gamma_neg})\"\n\nclass FocalLoss(nn.Module):\n    \"\"\"\n    Focal Loss for addressing class imbalance.\n    \n    Reference: Lin et al. \"Focal Loss for Dense Object Detection\" (2017)\n    \n    Args:\n        alpha: Weighting factor in [0, 1] to balance positive/negative examples\n               or a list of weights for each class\n        gamma: Focusing parameter for modulating loss (default: 2.0)\n        reduction: 'mean', 'sum', or 'none'\n    \"\"\"\n    def __init__(self, weights=[0.25, 0.75], alpha=None, gamma=2.0, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        alpha = alpha or (weights[0] / sum(weights))\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n    \n    def forward(self, inputs, targets):\n        \"\"\"\n        Args:\n            inputs: Predicted logits of shape (N, *) \n            targets: Ground truth labels of shape (N, *)\n        \"\"\"\n        # Apply sigmoid to get probabilities\n        probs = torch.sigmoid(inputs)\n        \n        # Calculate binary cross entropy\n        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n        \n        # Calculate focal term: (1 - p_t)^gamma\n        p_t = probs * targets + (1 - probs) * (1 - targets)\n        focal_weight = (1 - p_t) ** self.gamma\n        \n        # Apply alpha weighting\n        if self.alpha is not None:\n            if isinstance(self.alpha, (float, int)):\n                alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n            else:\n                alpha_t = self.alpha\n            focal_loss = alpha_t * focal_weight * bce_loss\n        else:\n            focal_loss = focal_weight * bce_loss\n        \n        # Apply reduction\n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        else:\n            return focal_loss\n\n    def __str__(self):\n        return f'FocalLoss(alpha={self.alpha}, gamma={self.gamma}, reduction=\\'{self.reduction}\\')'\n    def __repr__(self):\n        return str(self)\n\nclass HardNegativeMiningLoss(nn.Module):\n    \"\"\"\n    Hard Negative Mining for Binary Classification\n    \n    Keeps ALL positive samples and only the hardest negative samples.\n    Hardest = highest loss = most confusing to the model.\n    \"\"\"\n    def __init__(self, neg_ratio=3.0, pos_weight=16.0):\n        \"\"\"\n        Args:\n            neg_ratio: Number of hard negatives per positive (e.g., 3 means 3:1 ratio)\n            pos_weight: Weight multiplier for positive class to handle imbalance\n        \"\"\"\n        super().__init__()\n        self.neg_ratio = neg_ratio\n        self.pos_weight = pos_weight\n    \n    def forward(self, logits, targets):\n        \"\"\"\n        Args:\n            logits: Raw model outputs [batch_size, 1] or [batch_size]\n            targets: Ground truth labels [batch_size, 1] or [batch_size], values in {0, 1}\n        \n        Returns:\n            loss: Scalar tensor\n        \"\"\"\n        # Ensure correct shapes\n        logits = logits.squeeze()\n        targets = targets.squeeze().float()\n        \n        # Calculate loss for each sample individually (no reduction)\n        all_losses = F.binary_cross_entropy_with_logits(\n            logits, targets, reduction='none'\n        )\n        \n        # Separate positive and negative samples\n        pos_mask = targets == 1\n        neg_mask = targets == 0\n        \n        pos_losses = all_losses[pos_mask]\n        neg_losses = all_losses[neg_mask]\n        \n        # Handle edge cases\n        if pos_losses.numel() == 0 and neg_losses.numel() == 0:\n            return torch.tensor(0.0, device=logits.device, requires_grad=True)\n        \n        # === POSITIVE SAMPLES: Keep ALL (they're rare and important) ===\n        if pos_losses.numel() > 0:\n            # Apply positive class weight\n            weighted_pos_loss = (pos_losses * self.pos_weight).mean()\n            num_pos = pos_losses.numel()\n        else:\n            weighted_pos_loss = torch.tensor(0.0, device=logits.device)\n            num_pos = 0\n        \n        # === NEGATIVE SAMPLES: Keep only HARD ones ===\n        if neg_losses.numel() > 0 and num_pos > 0:\n            # Calculate how many hard negatives to keep\n            num_hard_neg = int(num_pos * self.neg_ratio)\n            num_hard_neg = min(num_hard_neg, neg_losses.numel())  # Can't exceed available negatives\n            \n            if num_hard_neg > 0:\n                # Select top-k hardest negatives (highest loss)\n                hard_neg_losses, _ = torch.topk(neg_losses, num_hard_neg)\n                neg_loss = hard_neg_losses.mean()\n            else:\n                neg_loss = neg_losses.mean()\n        elif neg_losses.numel() > 0:\n            # No positives in batch, use all negatives\n            neg_loss = neg_losses.mean()\n        else:\n            neg_loss = torch.tensor(0.0, device=logits.device)\n        \n        # Combine positive and negative losses\n        total_loss = weighted_pos_loss + neg_loss\n        \n        return total_loss\n\n    def __str__(self):\n        return f'HardNegativeMiningLoss(neg_ratio={self.neg_ratio}, pos_weight={self.pos_weight})'\n\n    def __repr__(self):\n        return str(self)\n\nclass AsymmetricFocalLoss(nn.Module):\n    def __init__(self, gamma_pos=1.0, gamma_neg=4.0, alpha=0.94, reduction='mean', epsilon=1e-7):\n        super().__init__()\n        self.gamma_pos = gamma_pos\n        self.gamma_neg = gamma_neg\n        self.alpha = alpha\n        self.reduction = reduction\n        self.epsilon = epsilon\n    \n    def forward(self, logits, targets):\n        logits = logits.squeeze()\n        targets = targets.squeeze().float()\n        probs = torch.sigmoid(logits)\n\n        probs = torch.clamp(probs, self.epsilon, 1 - self.epsilon)\n        \n        bce_loss = -(targets * torch.log(probs) + (1 - targets) * torch.log(1 - probs))\n\n        focal_weight = torch.where(\n            targets == 1,\n            (1 - probs) ** self.gamma_pos,  # Positive sample modulation\n            probs ** self.gamma_neg          # Negative sample modulation\n        )\n\n        alpha_weight = torch.where(\n            targets == 1,\n            torch.tensor(self.alpha, device=targets.device, dtype=targets.dtype),\n            torch.tensor(1 - self.alpha, device=targets.device, dtype=targets.dtype)\n        )\n\n        focal_loss = alpha_weight * focal_weight * bce_loss\n\n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        else:  # 'none'\n            return focal_loss\n\n    def __str__(self):\n        return f\"gamma_pos={self.gamma_pos}, gamma_neg={self.gamma_neg}, alpha={self.alpha}, epsilon={self.epsilon}, reduction=\\'{self.reduction}\\'\"\n    def __repr__(self):\n        return str(self)\n\nclass LogitAdjustedBCE(nn.Module):\n    \"\"\"\n    Logit-Adjusted Binary Cross Entropy Loss for long-tailed recognition.\n    \n    Adjusts the logits based on class frequencies to handle imbalanced datasets.\n    \n    Reference: Menon et al. \"Long-tail learning via logit adjustment\" (2021)\n    \n    Args:\n        pos_prior: Prior probability of positive class (frequency of 1s in training)\n        tau: Temperature parameter for adjustment (default: 1.0)\n        reduction: 'mean', 'sum', or 'none'\n    \"\"\"\n    def __init__(self, weights=[0.5, 0.5], tau=1.0, reduction='mean'):\n        super(LogitAdjustedBCE, self).__init__()\n        total = sum(weights)\n        self.neg_prior = weights[0] / total\n        self.pos_prior = weights[1] / total\n        self.tau = tau\n        self.reduction = reduction\n        \n        # Calculate log prior adjustment\n        self.logit_adjustment = torch.log(torch.tensor(self.pos_prior / self.neg_prior))\n    \n    def forward(self, inputs, targets):\n        \"\"\"\n        Args:\n            inputs: Predicted logits of shape (N, *)\n            targets: Ground truth labels of shape (N, *)\n        \"\"\"\n        # Move adjustment to same device as inputs\n        if self.logit_adjustment.device != inputs.device:\n            self.logit_adjustment = self.logit_adjustment.to(inputs.device)\n        \n        # Adjust logits based on class priors\n        adjusted_logits = inputs + self.tau * self.logit_adjustment\n        \n        # Calculate BCE loss with adjusted logits\n        loss = F.binary_cross_entropy_with_logits(\n            adjusted_logits, targets, reduction=self.reduction\n        )\n        \n        return loss\n    \n    def __str__(self):\n        return f'LogitAdjustedBCE(neg_prior={self.neg_prior}, pos_prior={self.pos_prior}, reduction=\\'{self.reduction}\\')'\n    def __repr__(self):\n        return str(self)","metadata":{"jupyter":{"source_hidden":true},"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T18:13:16.364423Z","iopub.execute_input":"2025-11-27T18:13:16.364692Z","iopub.status.idle":"2025-11-27T18:13:16.401648Z","shell.execute_reply.started":"2025-11-27T18:13:16.364674Z","shell.execute_reply":"2025-11-27T18:13:16.400906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train.py\nclass Trainer:\n    def __init__(\n        self,\n        title,\n        model,\n        optim,\n        loss_fn,\n        eval_metrics,\n        train_loader,\n        val_loader,\n        test_loader=None,\n        checkpointing=5,\n        lr_scheduler=None,\n        lr_step_frequency=200,\n        save_model_style=\"instance\", #state, instance\n        accumulate_gradient=1,\n        threshold=0.5,\n        dynamic_thresholding=True,\n        dynamic_thresholding_metric=\"accuracy\",\n        progress_bar_update=\"mean\",\n        device=\"cpu\"):\n\n        self.title = title\n        self.model = model.to(device)\n        self.optim = optim\n        self.loss_fn = loss_fn\n        self.eval_metrics = eval_metrics\n        self.accumulate_gradient = accumulate_gradient\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.test_loader = test_loader\n        self.threshold = threshold\n        self.dynamic_thresholding = dynamic_thresholding\n        self.dynamic_thresholding_metric = dynamic_thresholding_metric\n        self.progress_bar_update = progress_bar_update # laval, sum, avg\n        self.checkpointing = checkpointing\n        self.lr_scheduler = lr_scheduler\n        self.lr_step_frequency = lr_step_frequency\n        self.save_model_style = save_model_style\n        self.device = device\n        self.training_epoch = 1\n        self.validating_epoch = 1\n        self.lr_scheduler_step_counter = 1\n        self._last_dynamic_threshold = None\n\n        # initiate directories\n        self.title_dir = title\n        self.checkpoint_dir = os.path.join(title, \"checkpoints\")\n        self.train_records_dir = os.path.join(title, \"train_records\")\n        self.val_records_dir = os.path.join(title, \"val_records\")\n        self.test_records_dir = os.path.join(title, \"test_records\")\n        self.prediction_records_dir = os.path.join(title, \"prediction_records\")\n\n        os.makedirs(self.title_dir, exist_ok=True)\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n        os.makedirs(self.train_records_dir, exist_ok=True)\n        os.makedirs(self.val_records_dir, exist_ok=True)\n        os.makedirs(self.test_records_dir, exist_ok=True)\n        os.makedirs(self.prediction_records_dir, exist_ok=True)\n\n        os.makedirs(path.join(self.prediction_records_dir, \"train\"), exist_ok=True)\n        os.makedirs(path.join(self.prediction_records_dir, \"val\"), exist_ok=True)\n        os.makedirs(path.join(self.prediction_records_dir, \"test\"), exist_ok=True)\n        os.makedirs(path.join(self.train_records_dir, \"fixed\"), exist_ok=True)\n        os.makedirs(path.join(self.train_records_dir, \"dynamic\"), exist_ok=True)\n        os.makedirs(path.join(self.val_records_dir, \"fixed\"), exist_ok=True)\n        os.makedirs(path.join(self.val_records_dir, \"dynamic\"), exist_ok=True)\n\n        write_json({\n            \"model\" : str(model),\n            \"optimizer\" : str(optim),\n            \"loss_fn\" : str(loss_fn),\n            \"preprocessor\" : str(train_loader.dataset._transform),\n            \"augmentation\" : str(train_loader.dataset._augmentation),\n        }, path.join(self.title_dir, \"components.json\"))\n\n    def fit(self, epochs, train_verbose=False, val_verbose=False, summary_verbose=False, validating_frequency=5):\n        print(\"Training initiated...\")\n        for i in range(epochs - self.training_epoch):\n            self.train(train_verbose)\n\n            if (i + 1) % validating_frequency == 0:\n                self.val(val_verbose)\n\n            if (i + 1) % self.checkpointing == 0:\n                self.save([True, True], True)\n                self.save_performance(summary_verbose)\n\n    def train(self, verbose=False):\n        self.model.zero_grad()\n        self.model.train()\n        progress_bar = tqdm(enumerate(self.train_loader), total=len(self.train_loader), desc=f\"Training Epoch {self.training_epoch}...\")\n        \n        labels_list = []\n        predictions = []\n        loss_list = []\n        records = {}\n\n        for i, (images, labels) in progress_bar:\n            images = images.to(self.device)\n            labels = labels.to(self.device)\n\n            out = self.model(images).flatten()\n            labels = labels.flatten()\n            loss = self.loss_fn(out, labels)\n            loss.backward()\n\n            labels_list.append(labels.detach())\n            predictions.append(out.detach().sigmoid())\n            loss_list.append(loss.detach())\n\n            if (i + 1) % self.accumulate_gradient == 0 or i == len(self.train_loader) - 1 :\n                self.optim.step()\n                self.model.zero_grad()\n                self.lr_scheduler_step_counter += 1\n\n            if self.lr_scheduler and self.lr_scheduler_step_counter % self.lr_step_frequency == 0:\n                self.lr_scheduler.step()\n\n            if self.progress_bar_update == \"mean\":\n                update = torch.stack(loss_list).mean()\n            elif self.progress_bar_update == \"sum\":\n                update = torch.stack(loss_list).sum()\n            elif self.progress_bar_update == \"laval\":\n                update = loss_list[-1].item()\n\n            if self.lr_scheduler:\n                progress_bar.set_postfix({\"loss\":update.item(), \"lr\": self.lr_scheduler.get_last_lr()[0]})\n            else:\n                progress_bar.set_postfix({\"loss\":update.item()})\n\n        labels = torch.cat(labels_list).detach().cpu()\n        predictions = torch.cat(predictions).detach().cpu()\n        losses = torch.stack(loss_list)\n\n        fixed_scores, fixed_threshold = evaluate(labels, predictions, self.eval_metrics, self.threshold)\n        fixed_scores[\"loss\"] = losses.mean().item()\n        fixed_scores[\"threshold\"] = fixed_threshold\n        fixed_scores[\"epoch\"] = self.training_epoch\n        if self.lr_scheduler:\n            fixed_scores[\"learning_rate\"] = self.lr_scheduler.get_last_lr()[0]\n        records[\"fixed_threshold\"] = fixed_scores\n        write_json(fixed_scores, os.path.join(self.train_records_dir, \"fixed\", f\"records_{self.training_epoch}.json\"))\n\n        if self.dynamic_thresholding:\n            dynamic_scores, dynamic_threshold =  evaluate(labels, predictions, self.eval_metrics, self.dynamic_thresholding_metric)\n            dynamic_scores[\"loss\"] = losses.mean().item()\n            dynamic_scores[\"threshold\"] = dynamic_threshold\n            dynamic_scores[\"epoch\"] = self.training_epoch\n            if self.lr_scheduler:\n                dynamic_scores[\"learning_rate\"] = self.lr_scheduler.get_last_lr()[0]\n            records[\"dynamic_threshold\"] = dynamic_scores\n            write_json(dynamic_scores, os.path.join(self.train_records_dir, \"dynamic\", f\"records_{self.training_epoch}.json\"))\n\n        if verbose:\n            summarize_performance_table(records, width=20)\n\n        prediction_records = {\n            \"predictions\" : predictions.tolist(),\n            \"labels\" : labels.tolist()\n        }\n        write_json(prediction_records, path.join(self.prediction_records_dir, \"train\", f\"epoch_{self.training_epoch}_predictions.json\"))\n\n        self.training_epoch += 1\n\n    def eval_model(self, loader, verbose, threshold):\n        validating_epoch = self.validating_epoch\n        val_loader = self.val_loader\n        dynamic_thresholding = self.dynamic_thresholding\n        _threshold = self.threshold\n        self.threshold = threshold\n        self.dynamic_thresholding = False\n        self.validating_epoch = -1\n        self.val_loader = loader\n        try:\n            self.val(verbose)\n        except:\n            self.val_loader = val_loader\n            self.validating_epoch = validating_epoch\n            self.dynamic_thresholding = dynamic_thresholding\n            self.threshold = _threshold\n        finally:\n            self.val_loader = val_loader\n            self.validating_epoch = validating_epoch\n            self.dynamic_thresholding = dynamic_thresholding\n            self.threshold = _threshold\n            \n        \n    \n    @torch.no_grad()\n    def val(self, verbose=False):\n        self.model.eval()\n        progress_bar = tqdm(self.val_loader, total=len(self.val_loader), desc=f\"validating Epoch {self.validating_epoch}...\")\n\n        labels_list = []\n        predictions = []\n        loss_list = []\n        records = {}\n\n        for images, labels in progress_bar:\n            images = images.to(self.device)\n            labels = labels.to(self.device)\n\n            out = self.model(images).flatten()\n            labels = labels.flatten()\n            loss = self.loss_fn(out, labels)\n\n            labels_list.append(labels.detach())\n            predictions.append(out.detach().sigmoid())\n            loss_list.append(loss.detach())\n\n            if self.progress_bar_update == \"mean\":\n                update = torch.stack(loss_list).mean()\n            elif self.progress_bar_update == \"sum\":\n                update = torch.stack(loss_list).sum()\n            elif self.progress_bar_update == \"laval\":\n                update = loss_list[-1]\n\n            progress_bar.set_postfix({\"loss\":update.item()})\n\n        labels = torch.cat(labels_list).detach().cpu()\n        predictions = torch.cat(predictions).detach().cpu()\n        losses = torch.stack(loss_list)\n\n        fixed_scores, fixed_threshold = evaluate(labels, predictions, self.eval_metrics, self.threshold)\n        fixed_scores[\"loss\"] = losses.mean().item()\n        fixed_scores[\"threshold\"] = fixed_threshold\n        fixed_scores[\"epoch\"] = self.validating_epoch\n        records[\"fixed_threshold\"] = fixed_scores\n        write_json(fixed_scores, os.path.join(self.val_records_dir, \"fixed\", f\"records_{self.validating_epoch}.json\"))\n\n        if self.dynamic_thresholding:\n            dynamic_scores, dynamic_threshold =  evaluate(labels, predictions, self.eval_metrics, self.dynamic_thresholding_metric)\n            dynamic_scores[\"loss\"] = losses.mean().item()\n            dynamic_scores[\"threshold\"] = dynamic_threshold\n            dynamic_scores[\"epoch\"] = self.validating_epoch\n            records[\"dynamic_threshold\"] = dynamic_scores\n            self._last_dynamic_threshold = dynamic_threshold\n            write_json(dynamic_scores, os.path.join(self.val_records_dir, \"dynamic\", f\"records_{self.validating_epoch}.json\"))\n            \n        if verbose:\n            summarize_performance_table(records, width=20)\n\n        prediction_records = {\n            \"predictions\" : predictions.tolist(),\n            \"labels\" : labels.tolist()\n        }\n        write_json(prediction_records, path.join(self.prediction_records_dir, \"val\", f\"epoch_{self.validating_epoch}_predictions.json\"))\n\n        try:\n            self.validating_epoch += 1\n        except:\n            print(\"validating_epoch is set to non-integer\")\n\n    @torch.no_grad()\n    def test(self, verbose=False):\n        self.model.eval()\n        progress_bar = tqdm(self.test_loader, total=len(self.test_loader), desc=f\"Testing...\")\n\n        labels_list = []\n        predictions = []\n        loss_list = []\n        records = {}\n\n        for images, labels in progress_bar:\n            images = images.to(self.device)\n            labels = labels.to(self.device)\n\n            out = self.model(images).flatten()\n            labels = labels.flatten()\n            loss = self.loss_fn(out, labels)\n\n            labels_list.append(labels.detach())\n            predictions.append(out.detach().sigmoid())\n            loss_list.append(loss.detach())\n\n            if self.progress_bar_update == \"mean\":\n                update = torch.stack(loss_list).mean()\n            elif self.progress_bar_update == \"sum\":\n                update = torch.stack(loss_list).sum()\n            elif self.progress_bar_update == \"laval\":\n                update = loss_list[-1]\n\n            progress_bar.set_postfix({\"loss\":update.item()})\n\n        labels = torch.cat(labels_list).detach().cpu()\n        predictions = torch.cat(predictions).detach().cpu()\n        losses = torch.stack(loss_list)\n\n        fixed_scores, fixed_threshold = evaluate(labels, predictions, self.eval_metrics, self.threshold)\n        fixed_scores[\"loss\"] = losses.mean().item()\n        fixed_scores[\"threshold\"] = fixed_threshold\n        fixed_scores[\"epoch\"] = self.validating_epoch\n        records[\"fixed_threshold\"] = fixed_scores\n        write_json(fixed_scores, os.path.join(self.test_records_dir, f\"fixed_record.json\"))\n            \n        if verbose:\n            summarize_performance_table(records, width=20)\n\n        prediction_records = {\n            \"predictions\" : predictions.tolist(),\n            \"labels\" : labels.tolist()\n        }\n\n        write_json(prediction_records, path.join(self.prediction_records_dir, \"test\", f\"predictions.json\"))\n\n    def save_weights(self, step_back=False):\n        ckpt = {\n            \"model\" : self.model,\n            \"optim\" : self.optim\n        }\n        if self.save_model_style == \"state\":\n            torch.save({k:v.state_dict() for (k, v) in ckpt.items()}, path.join(self.checkpoint_dir, f\"ckpt_{self.training_epoch - int(step_back)}.pth\"))\n        if self.save_model_style == \"instance\":\n            torch.save(ckpt, path.join(self.checkpoint_dir, f\"ckpt_{self.training_epoch - int(step_back)}.pt\"))\n\n    def save_config(self, step_back=[False, False]):\n        config = {\n        \"title\" : self.title,\n        \"weights\" : None if len(os.listdir(self.checkpoint_dir)) == 0 else path.join(self.checkpoint_dir, max(os.listdir(self.checkpoint_dir), key=lambda x: int(x.split(\"_\")[1].split(\".\")[0]) )),\n        \"accumulate_gradient\" : self.accumulate_gradient,\n        \"threshold\" : self.threshold,\n        \"dynamic_thresholding\" : self.dynamic_thresholding,\n        \"dynamic_thresholding_metric\" : self.dynamic_thresholding_metric,\n        \"progress_bar_update\" : self.progress_bar_update,\n        \"checkpointing\" : self.checkpointing,\n        \"device\" : self.device,\n        \"training_epoch\" : self.training_epoch - int(step_back[0]),\n        \"validating_epoch\" : self.validating_epoch - int(step_back[1]),\n        \"save_model_style\" : self.save_model_style\n        }\n        write_json(config, path.join(self.title_dir, \"config.json\"))\n\n    def from_config(directory, instances={}, back_step=False):\n        config = read_json(directory)\n\n        weights = torch.load(config[\"weights\"], weights_only=False)\n        weights_dict = instances[\"weights\"]\n        training_epoch = config[\"training_epoch\"]\n        validating_epoch = config[\"validating_epoch\"]\n\n        del instances[\"weights\"]\n        del config[\"weights\"]\n        del config[\"training_epoch\"]\n        del config[\"validating_epoch\"]\n\n        for key, weight in weights.items():\n            if config[\"save_model_style\"] == \"state\":\n                weights_dict[key].load_state_dict(weight)\n            elif config[\"save_model_style\"] == \"instance\":\n                weights_dict[key] = weight\n\n        trainer = Trainer(**weights_dict, **config, **instances)\n        trainer.training_epoch = training_epoch - int(back_step)\n        trainer.validating_epoch = validating_epoch\n\n        return trainer\n\n    def save(self, config_step_back=[False, False], weights_step_back=False):\n        self.save_weights(weights_step_back)\n        self.save_config(config_step_back)\n        print(\"model weights and config has been saved successfully\")\n\n    def read_performance(self):\n        \n        # section\n        train_fixed = sorted([\n            read_json(path.join(self.train_records_dir, \"fixed\", file))\n            for file in os.listdir(path.join(self.train_records_dir, \"fixed\"))\n            ], key=lambda x: x[\"epoch\"])\n        train_dynamic = sorted([\n            read_json(path.join(self.train_records_dir, \"dynamic\", file))\n            for file in os.listdir(path.join(self.train_records_dir, \"dynamic\"))\n            ], key=lambda x: x[\"epoch\"])\n\n        val_fixed = sorted([\n            read_json(path.join(self.val_records_dir, \"fixed\", file))\n            for file in os.listdir(path.join(self.val_records_dir, \"fixed\"))\n            ], key=lambda x: x[\"epoch\"])\n        val_dynamic = sorted([\n            read_json(path.join(self.val_records_dir, \"dynamic\", file))\n            for file in os.listdir(path.join(self.val_records_dir, \"dynamic\"))\n            ], key=lambda x: x[\"epoch\"])\n        \n        return {\n            \"train_fixed\": train_fixed, \n            \"train_dynamic\": train_dynamic,\n            \"val_fixed\": val_fixed,\n            \"val_dynamic\": val_dynamic\n        }\n\n    def save_performance(self, verbose=False): # read_performance\n        performance_dict = self.read_performance()\n        for key, performance in performance_dict.items():\n            try:\n                visualize_dict_list(\n                    performance,\n                    path.join(self.title_dir, f\"{key}_records_summary.png\"),\n                    show_summary=verbose,\n                    title=f\"{key.replace('_', ' ')} threshold summary\",\n                    exclude=[\"epoch\"]\n                    )\n            except:\n                print(f\"no records for {key.replace('_', ' ')}\")\n\n    def show_performance(self, threshold_mode=\"both\", iteration_mode=\"both\"):\n        assert threshold_mode in [\"dynamic\", \"fixed\", \"both\"]\n        assert iteration_mode in [\"train\", \"val\", \"both\"]\n\n        threshold_mode = \"\" if threshold_mode == \"both\" else threshold_mode\n        iteration_mode = \"\" if iteration_mode == \"both\" else iteration_mode\n\n        performance_dict = self.read_performance()\n        \n        for key, performance in performance_dict.items():\n            if threshold_mode in key and iteration_mode in key:\n                visualize_dict_list(\n                    performance,\n                    None,\n                    show_summary=True,\n                    exclude=[\"epoch\"]\n                    )\n\n    def to(self, device):\n        self.device = device\n\nclass ClassifierTrainer:\n    def __init__(\n        self,\n        title,\n        model,\n        eval_metrics,\n        train_dataset,\n        val_dataset,\n        test_dataset,\n        threshold=0.5,\n        dynamic_thresholding=True,\n        dynamic_thresholding_metric=\"accuracy\"\n        ):\n\n        self.title = title\n        self.model = model\n        self.eval_metrics = eval_metrics\n        self.threshold = threshold\n        self.dynamic_thresholding = dynamic_thresholding\n        self.dynamic_thresholding_metric = dynamic_thresholding_metric\n\n        # initiate directories\n        self.title_dir = title\n        self.save_model_dir = os.path.join(title, \"model.pk\")\n        self.train_records_dir = os.path.join(title, \"train_records\")\n        self.val_records_dir = os.path.join(title, \"val_records\")\n        self.test_records_dir = os.path.join(title, \"test_records\")\n        self.prediction_records_dir = os.path.join(title, \"prediction_records\")\n\n        os.makedirs(self.title_dir, exist_ok=True)\n        os.makedirs(self.train_records_dir, exist_ok=True)\n        os.makedirs(self.val_records_dir, exist_ok=True)\n        os.makedirs(self.test_records_dir, exist_ok=True)\n        os.makedirs(self.prediction_records_dir, exist_ok=True)\n\n        if isinstance(train_dataset, str):\n            self.x_train, self.y_train = torch.load(train_dataset, weights_only=False, map_location=\"cpu\")\n        else:\n            self.x_train, self.y_train = self.tensorize(train_dataset)\n\n        self.x_train = self.x_train.reshape(self.x_train.shape[0], -1)\n\n        if isinstance(val_dataset, str):\n            self.x_val, self.y_val = torch.load(val_dataset, weights_only=False, map_location=\"cpu\")\n        else:\n            self.x_val, self.y_val = self.tensorize(val_dataset)\n\n        self.x_val = self.x_val.reshape(self.x_val.shape[0], -1)\n\n        if isinstance(test_dataset, str):\n            self.x_test, self.y_test = torch.load(test_dataset, weights_only=False, map_location=\"cpu\")\n        else:\n            self.x_test, self.y_test = self.tensorize(test_dataset)\n\n        self.x_test = self.x_test.reshape(self.x_test.shape[0], -1)\n\n        write_json({\n            \"model\" : str(model)\n        }, path.join(self.title_dir, \"components.json\"))\n\n    def fit(self, train_verbose=False, val_verbose=False, test_verbose=False):\n        print(\"Training initiated...\")\n        self.train(train_verbose)\n        print(\"validating initiated...\")\n        self.val(val_verbose)\n        print(\"testing initiated...\")\n        threshold = read_json(os.path.join(self.val_records_dir, f\"dynamic_record.json\"))[\"threshold\"]\n        self.test(test_verbose, threshold=threshold)\n\n        self.save()\n        self.show_performance(20)\n\n    def train(self, verbose=False):\n        self.model.fit(self.x_train, self.y_train)\n        predictions = self.model(self.x_train).flatten()\n        labels = self.y_train.detach().flatten()\n\n        fixed_scores, fixed_threshold = evaluate(labels, predictions, self.eval_metrics, self.threshold)\n        fixed_scores[\"threshold\"] = fixed_threshold\n        write_json(fixed_scores, os.path.join(self.train_records_dir, f\"fixed_record.json\"))\n\n        if self.dynamic_thresholding:\n            dynamic_scores, dynamic_threshold =  evaluate(labels, predictions, self.eval_metrics, self.dynamic_thresholding_metric)\n            dynamic_scores[\"threshold\"] = dynamic_threshold\n            write_json(dynamic_scores, os.path.join(self.train_records_dir, f\"dynamic_record.json\"))\n\n        if verbose:\n            print_run_summary(fixed_scores, fixed_threshold)\n\n            if self.dynamic_thresholding:\n                print_run_summary(dynamic_scores, dynamic_threshold)\n\n        prediction_records = {\n            \"predictions\" : predictions.tolist(),\n            \"labels\" : labels.tolist()\n        }\n        write_json(prediction_records, path.join(self.prediction_records_dir, f\"train_predictions.json\"))\n\n    def tensorize(self, dataset):\n        x = []\n        y = []\n\n        for data, label in tqdm(dataset, desc=\"tensorizing...\"):\n            x.append(data.cpu())\n            y.append(label.cpu())\n        \n        x = torch.stack(x)\n        y = torch.stack(y)\n        return x, y\n\n    @torch.no_grad()\n    def val(self, verbose=False):\n        predictions = self.model(self.x_val).flatten()\n        labels = self.y_val.detach().flatten()\n\n        fixed_scores, fixed_threshold = evaluate(labels, predictions, self.eval_metrics, self.threshold)\n        fixed_scores[\"threshold\"] = fixed_threshold\n        write_json(fixed_scores, os.path.join(self.val_records_dir, f\"fixed_record.json\"))\n\n        if self.dynamic_thresholding:\n            dynamic_scores, dynamic_threshold =  evaluate(labels, predictions, self.eval_metrics, self.dynamic_thresholding_metric)\n            dynamic_scores[\"threshold\"] = dynamic_threshold\n            write_json(dynamic_scores, os.path.join(self.val_records_dir, f\"dynamic_record.json\"))\n\n        if verbose:\n            print_run_summary(fixed_scores, fixed_threshold)\n\n            if self.dynamic_thresholding:\n                print_run_summary(dynamic_scores, dynamic_threshold)\n\n        prediction_records = {\n            \"predictions\" : predictions.tolist(),\n            \"labels\" : labels.tolist()\n        }\n        write_json(prediction_records, path.join(self.prediction_records_dir, f\"val_predictions.json\"))\n\n    @torch.no_grad()\n    def test(self, threshold=None, verbose=False):\n        predictions = self.model(self.x_test).flatten()\n        labels = self.y_test.detach().flatten()\n\n        fixed_scores, fixed_threshold = evaluate(labels, predictions, self.eval_metrics, threshold or self.threshold)\n        fixed_scores[\"threshold\"] = fixed_threshold\n        write_json(fixed_scores, os.path.join(self.test_records_dir, f\"fixed_record.json\"))\n\n        if verbose:\n            print_run_summary(fixed_scores, fixed_threshold)\n\n            if self.dynamic_thresholding:\n                print_run_summary(dynamic_scores, dynamic_threshold)\n\n        prediction_records = {\n            \"predictions\" : predictions.tolist(),\n            \"labels\" : labels.tolist()\n        }\n        write_json(prediction_records, path.join(self.prediction_records_dir, f\"test_predictions.json\"))\n\n    def save_weights(self):\n        self.model.save(self.save_model_dir)\n\n    def save_config(self):\n        config = {\n        \"title\" : self.title,\n        \"weights\" : self.save_model_dir,\n        \"threshold\" : self.threshold,\n        \"dynamic_thresholding\" : self.dynamic_thresholding,\n        \"dynamic_thresholding_metric\" : self.dynamic_thresholding_metric,\n        }\n        write_json(config, path.join(self.title_dir, \"config.json\"))\n\n    def from_config(\n        directory,\n        model,\n        eval_metrics,\n        train_dataset,\n        val_dataset\n        ):\n        config = read_json(directory)\n\n        config[\"model\"] = model.load(config[\"weights\"])\n        config[\"eval_metrics\"] = eval_metrics\n        config[\"train_dataset\"] = train_dataset\n        config[\"val_dataset\"] = val_dataset\n\n        del config[\"weights\"]\n\n        trainer = ClassifierTrainer(**config)\n\n        return trainer\n\n    def save(self):\n        self.save_weights()\n        self.save_config()\n        print(\"model weights and config has been saved successfully\")\n\n    def read_performance(self):\n        train_fixed = path.join(self.train_records_dir, \"fixed_record.json\")\n        train_dynamic  = path.join(self.train_records_dir, \"dynamic_record.json\")\n        val_fixed  = path.join(self.val_records_dir, \"fixed_record.json\")\n        val_dynamic  = path.join(self.val_records_dir, \"dynamic_record.json\")\n        \n        return {\n            \"train_fixed\": train_fixed, \n            \"train_dynamic\": train_dynamic,\n            \"val_fixed\": val_fixed,\n            \"val_dynamic\": val_dynamic\n        }\n\n    def show_performance(self, width=15):\n        performances = self.read_performance()\n        performances = {key:read_json(p) for key, p in performances.items()}\n\n        titles = [col_name.center(width) for col_name in performances.keys()]\n\n        s = (\"  \" + \"_\" * (width)) * 5 + \"  \"\n        print(s)\n        print(\"||\" + \"||\".join([\"metric\".center(width), *titles]) + \"||\")\n        print(s.replace(\"  \", \"||\"))\n\n        for metric in performances[max(performances, key=lambda x: len(performances[x].keys()))].keys():\n            lst = []\n            for key in performances.keys():\n                score = performances[key].get(metric, \"Nan\")\n                score = round(score, 4) if isinstance(score, float) else score\n                score = str(score)\n                lst.append(score)\n            print(\"||\" + \"||\".join([metric.center(width), *[x.center(width) for x in lst]]) + \"||\")\n            print(s.replace(\"  \", \"||\"))\n\n# do not use this function\n# def train_wavelength(\n#     config,\n#     wavelength,\n#     preprocessor,\n#     augmentation,\n#     eval_metrics,\n#     loss_fn,\n#     augmentation_class=\"positive\",\n#     epochs=100,\n#     verboses=[False, False, False],\n#     validating_frequency=10\n#     ):\n#     train_dataset = WaveLenghtDataset(\n#         config.train_data_dir,\n#         process_meta_data(config.train_meta_dir),\n#         wavelength,\n#         preprocessor,\n#         augmentation,\n#         augmentation_class=augmentation_class\n#         )\n\n#     val_dataset = WaveLenghtDataset(\n#         config.val_data_dir,\n#         process_meta_data(config.val_meta_dir),\n#         wavelength,\n#         preprocessor\n#         )\n\n#     sub_title = path.join(config.title, wavelength)\n#     model = efficientnet_v2_s(EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n#     model.classifier = nn.Sequential(\n#         nn.Dropout(0.5),  # Increase dropout\n#         nn.Linear(model.classifier[1].in_features, 1)\n#     )\n#     optim = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n#     train_loader = DataLoader(train_dataset, batch_size=config.batch_size)\n#     val_loader = DataLoader(val_dataset, batch_size=config.batch_size)\n\n#     trainer = Trainer(\n#             sub_title,\n#             model,\n#             optim,\n#             loss_fn,\n#             eval_metrics,\n#             train_loader,\n#             val_loader,\n#             checkpointing=validating_frequency,\n#             accumulate_gradient=max(1, int(config.batch_size / config.effective_batch_size)),\n#             threshold=0.5,\n#             dynamic_thresholding=True,\n#             dynamic_thresholding_metric=config.target_metric,\n#             progress_bar_update=\"mean\",\n#             device=\"cuda\"\n#             )\n\n#     trainer.fit(\n#         epochs=epochs,\n#         train_verbose=verboses[0],\n#         val_verbose=verboses[1],\n#         summary_verbose=verboses[2],\n#         validating_frequency=validating_frequency\n#         )\n\n# def get_instance(\n#     wavelength,\n#     config,\n#     preprocessor,\n#     augmentation,\n#     eval_metrics\n#     ):\n#     train_dataset = WaveLenghtDataset(\n#         config.train_data_dir,\n#         process_meta_data(config.train_meta_dir),\n#         wavelength,\n#         preprocessor,\n#         augmentation\n#         )\n\n#     val_dataset = WaveLenghtDataset(\n#         config.train_data_dir,\n#         process_meta_data(config.val_meta_dir),\n#         wavelength,\n#         preprocessor\n#         )\n#     model = efficientnet_v2_s(EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n#     model.classifier = nn.Sequential(\n#         nn.Dropout(0.3),  # Increase dropout\n#         nn.Linear(model.classifier[1].in_features, 1)\n#     )\n#     optim = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n#     loss_fn = torch.nn.BCEWithLogitsLoss()\n#     train_loader = DataLoader(train_dataset, batch_size=config.batch_size)\n#     val_loader = DataLoader(val_dataset, batch_size=config.batch_size)\n    \n#     return {\n#         \"weights\" : {\n#             \"model\" : model,\n#             \"optim\" : optim\n#             },\n#         \"loss_fn\" : loss_fn,\n#         \"eval_metrics\" : eval_metrics,\n#         \"train_loader\" : train_loader,\n#         \"val_loader\" : val_loader\n#         }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T18:13:16.402540Z","iopub.execute_input":"2025-11-27T18:13:16.402739Z","iopub.status.idle":"2025-11-27T18:13:16.463200Z","shell.execute_reply.started":"2025-11-27T18:13:16.402723Z","shell.execute_reply":"2025-11-27T18:13:16.462447Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# config.py\nclass Config:\n    def __init__(self):\n        # -----------------------------\n        # Paths\n        # -----------------------------\n        self.title = \"/kaggle/working/pure_original_dataset\"\n        self.base_route = \"/kaggle/input/sdobenchmark/SDOBenchmark_full\"\n        self.seed = 42\n\n        self.train_data_dir = path.join(self.base_route, \"training\")\n        self.test_data_dir = path.join(self.base_route, \"test\")\n\n        self.train_meta_dir = path.join(self.train_data_dir, \"meta_data.csv\")\n        self.test_meta_dir = path.join(self.test_data_dir, \"meta_data.csv\")\n\n        self.cache_dir = path.join(self.title, \"cache\")\n\n        self.synthesized_images_dir = None    # provided externally \n        # ratio if between (0,1) else will be a fixed number, num of\n        # number of positive examples in original dataset is 510\n        self.synthesized_sample_size = None # int(510 * 0.50)\n\n        # -----------------------------\n        # Data Settings\n        # -----------------------------\n        self.wavelengths = [\n            \"94\", \"131\", \"171\", \"193\", \"211\",\n            \"304\", \"335\", \"1700\", \"continuum\", \"magnetogram\"\n        ]\n        self.progress_bar_update = \"mean\"\n\n        self.dynamic_thresholding = True\n        self.checkpointing = 2\n        self.threshold = 0.5\n\n        self.image_size = 128\n        self.batch_size = 16\n        self.effective_batch_size = 32\n        self.oversampling_ratio = [0.9, 0.1] # # minority/majority ratio\n        self.embed_dim = 1280\n\n        # -----------------------------\n        # Fusion Model Training\n        # -----------------------------\n        self.fusion_num_epochs = 20\n        self.fusion_batch_size = 16\n        self.fusion_learning_rate = 1e-3\n\n        # -----------------------------\n        # Classifier Training\n        # -----------------------------\n        self.use_smote = True\n        self.classifier = \"xgboost\"\n        self.learning_rate = 5e-5\n        self.weight_decay = 0.05\n        self.num_epochs = 24\n        self.target_metric = \"tss\"\n\n        # -----------------------------\n        # Execution Flags\n        # -----------------------------\n        self.PURNE_CHECKPOINTS = False\n        self.TRAIN_CLASSIFIERS = False\n        self.ENCODE = False\n        self.TRAIN_FUSION = True\n        self.TRAIN_CLASSIFIER = True\n\n        # -----------------------------\n        # System\n        # -----------------------------\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # -----------------------------------------\n    # JSON export\n    # -----------------------------------------\n    def to_json(self):\n        config = {\n            \"base_route\": self.base_route,\n            \"title\": self.title,\n            \"train_data_dir\": self.train_data_dir,\n            \"test_data_dir\": self.test_data_dir,\n            \"train_meta_dir\": self.train_meta_dir,\n            \"test_meta_dir\": self.test_meta_dir,\n            \"cache_dir\": self.cache_dir,\n\n            \"wavelengths\": self.wavelengths,\n            \"image_size\": self.image_size,\n            \"batch_size\": self.batch_size,\n            \"effective_batch_size\": self.effective_batch_size,\n            \"oversampling_ratio\": self.oversampling_ratio,\n\n            \"dynamic_thresholding\" : self.dynamic_thresholding,\n            \"checkpointing\" : self.checkpointing,\n            \"threshold\" : self.threshold,\n\n            \"fusion_num_epochs\": self.fusion_num_epochs,\n            \"fusion_batch_size\": self.fusion_batch_size,\n            \"fusion_learning_rate\": self.fusion_learning_rate,\n\n            \"classifier\": self.classifier,\n            \"use_smote\": self.use_smote,\n            \"learning_rate\": self.learning_rate,\n            \"num_epochs\": self.num_epochs,\n            \"target_metric\": self.target_metric,\n\n            \"TRAIN_CLASSIFIERS\": self.TRAIN_CLASSIFIERS,\n            \"TRAIN_CLASSIFIER\": self.TRAIN_CLASSIFIER,\n            \"TRAIN_FUSION\": self.TRAIN_FUSION,\n            \"ENCODE\": self.ENCODE,\n            \"PURNE_CHECKPOINTS\": self.PURNE_CHECKPOINTS,\n\n            \"device\": self.device,\n            \"synthesized_images_dir\": self.synthesized_images_dir,\n            \"synthesized_sample_size\": self.synthesized_sample_size\n        }\n\n        write_json(config, path.join(self.title, \"global_config.json\"))\n\nconfig = Config()\nset_global_seed(config.seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T18:13:16.464200Z","iopub.execute_input":"2025-11-27T18:13:16.464477Z","iopub.status.idle":"2025-11-27T18:13:16.506743Z","shell.execute_reply.started":"2025-11-27T18:13:16.464452Z","shell.execute_reply":"2025-11-27T18:13:16.505988Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Distribution\nmeta = process_meta_data(config.train_meta_dir)\ntest_meta = process_meta_data(config.test_meta_dir)\n\ntrain_meta, val_meta = train_test_split(\n    meta,\n    test_size=0.1,\n    stratify=meta[\"peak_flux\"],\n    random_state=config.seed\n)\n\n\nimg, train_freq, val_freq, test_freq = plot_frequency_bars(\n    [train_meta[\"peak_flux\"], val_meta[\"peak_flux\"], test_meta[\"peak_flux\"]],\n    [\"non-severe\", \"severe\"],\n    show_img=True\n    )\n\nprint(\"training dataset distribution...\", train_freq)\nprint(\"validation dataset distribution...\", val_freq)\nprint(\"testing dataset distribution...\", test_freq)\n\nos.makedirs(config.title, exist_ok=True)\nimg.save(path.join(config.title, \"data_distribution.png\"))\nconfig.to_json()","metadata":{"jupyter":{"source_hidden":true},"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T18:13:16.507582Z","iopub.execute_input":"2025-11-27T18:13:16.507957Z","iopub.status.idle":"2025-11-27T18:13:17.067148Z","shell.execute_reply.started":"2025-11-27T18:13:16.507905Z","shell.execute_reply":"2025-11-27T18:13:17.066500Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Preprocessor & Augmentation & Loss Function & Evaluation Mertrics\npreprocessor = T.Compose([\n    ReadImgs(\"stack\"),\n    T.Resize((config.image_size, config.image_size)),\n    T.ConvertImageDtype(dtype=torch.float32),\n    FrequencyChannelTransform(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\naugmentation = T.Compose([\n    T.RandomHorizontalFlip(p=0.5),\n    T.RandomVerticalFlip(p=0.5),\n    # T.RandomRotation(degrees=[0, 90, 180, 270]),  # 90 increments only\n    T.RandomChoice([  # Apply one of these\n        T.ColorJitter(brightness=0.02, contrast=0.02),  # Subtle intensity changes\n        T.GaussianBlur(kernel_size=3, sigma=(0.01, 0.03)),  # Mild smoothing\n    ]),\n])\n\n# loss_fn = HybridLossFunction(\n#     FocalLoss(weights=[train_freq[0], train_freq[1] + (config.synthesized_sample_size or 0)], gamma=4),\n#     LogitAdjustedBCE(weights=[train_freq[0], train_freq[1] + (config.synthesized_sample_size or 0)], tau=1.0),\n#     weights=[0.7, 0.3]\n#     )\n# loss_fn = InverseFreqWeightedBCE([train_freq[0], train_freq[1] + (config.synthesized_sample_size or 0)])\n# loss_fn = FocalLoss(alpha=0.939, gamma=2.0)\n# loss_fn = AsymmetricFocalLoss(gamma_pos=1.4, gamma_neg=4.0, alpha=0.90)\nloss_fn = HybridLossFunction(\n    AsymmetricFocalLoss(gamma_pos=1.4, gamma_neg=4.0, alpha=0.93),\n    HardNegativeMiningLoss(neg_ratio=2.0, pos_weight=train_freq[0] / train_freq[1]),\n    weights=[.9, .1]\n)\n\neval_metrics = {\n    \"accuracy\": accuracy_score,\n    \"precision\": precision_score,\n    \"recall\": recall_score,\n    \"f1\": f1_score,\n    \"mcc\": matthews_corrcoef,\n    \"tss\": true_skill_statistic,\n    \"roc_auc\": roc_auc_score,\n    \"far\": far_score,\n    \"csi\": csi_score,\n    \"hss\": hss_score\n}\n\nwrite_note(\n\"\"\"\nused WeightedRandomSampler where each class weight is it's frequency\nutilized full timesteps instead of picking one and ditching the rest\n\"\"\"[1:], path.join(config.title, \"dev_note.txt\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T18:13:17.067870Z","iopub.execute_input":"2025-11-27T18:13:17.068173Z","iopub.status.idle":"2025-11-27T18:13:17.075339Z","shell.execute_reply.started":"2025-11-27T18:13:17.068145Z","shell.execute_reply":"2025-11-27T18:13:17.074591Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training each classifier\nif config.TRAIN_CLASSIFIERS:\n    pick_up_progress = False\n\n    # don't cahnge manually\n    resume_wavelength = False\n\n    if pick_up_progress:\n        wavelengths = [wavelength for wavelength in config.wavelengths if wavelength not in os.listdir(config.title)]\n        incompleted_wavelength = check_incomplete(path.join(config.title), config.num_epochs)\n        wavelengths = incompleted_wavelength + wavelengths\n        resume_wavelength = len(incompleted_wavelength) >= 1 \n    else:\n        wavelengths = config.wavelengths\n\n    for wavelength in wavelengths:\n        print(f\"current wavelength: {wavelength}\")\n\n        real_dataset = WaveLenghtDatasetV2(\n            config.train_data_dir,\n            train_meta,\n            wavelength,\n            preprocessor,\n            augmentation\n            )\n\n        if config.synthesized_sample_size is not None \\\n            and config.synthesized_images_dir is not None:\n            synthesized_dataset = SynthesizedDataset(\n                config.synthesized_images_dir,\n                wavelength,\n                synthesized_class=1,\n                transform=preprocessor,\n                augmentation=augmentation,\n                num_samples=config.synthesized_sample_size\n            )\n            train_dataset = MergedDatasets(real_dataset, synthesized_dataset)\n            weights = torch.cat([\n                torch.tensor(list(real_dataset._metadata[\"peak_flux\"].iloc())).int(),\n                torch.ones(len(synthesized_dataset))\n                ]).int()\n        else:\n            train_dataset = real_dataset\n            weights = torch.tensor(list(real_dataset._metadata[\"peak_flux\"].iloc())).int()\n\n        val_dataset = WaveLenghtDatasetV2(\n            config.train_data_dir,\n            val_meta,\n            wavelength,\n            preprocessor\n            )\n\n        test_dataset = WaveLenghtDatasetV2(\n            config.test_data_dir,\n            test_meta,\n            wavelength,\n            preprocessor\n        )\n\n        sampler = WeightedRandomSampler(\n            weights=(weights.bincount() * (torch.tensor(config.oversampling_ratio) if config.oversampling_ratio is not None else 1))[weights],\n            num_samples=len(weights),\n            replacement=True\n        )\n\n        os.makedirs(config.cache_dir, exist_ok=True)\n\n        sub_title = path.join(config.title, wavelength)\n        model = efficientnet_v2_s(EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n        model.classifier = nn.Sequential(\n            nn.Dropout(0.5),  # Increase dropout\n            nn.Linear(model.classifier[1].in_features, 1)\n        )\n\n        optim = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, sampler=sampler)\n        val_loader = DataLoader(val_dataset, batch_size=config.batch_size)\n        test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n\n        if resume_wavelength:\n            resume_wavelength = False\n            trainer = Trainer.from_config(\n                path.join(config.title, wavelength, \"config.json\"),\n                get_instance(\n                    wavelength,\n                    config,\n                    preprocessor,\n                    augmentation,\n                    eval_metrics,\n                    ),\n                back_step=True\n            )\n        else:\n            trainer = Trainer(\n                    sub_title,\n                    model,\n                    optim,\n                    loss_fn,\n                    eval_metrics,\n                    train_loader,\n                    val_loader,\n                    test_loader\n                    checkpointing=config.checkpointing,\n                    accumulate_gradient=max(1, int(config.batch_size / config.effective_batch_size)),\n                    threshold=config.threshold,\n                    dynamic_thresholding=config.dynamic_thresholding,\n                    dynamic_thresholding_metric=config.target_metric,\n                    progress_bar_update=config.progress_bar_update,\n                    device=config.device\n                    )\n\n        trainer.fit(\n            epochs=config.num_epochs,\n            train_verbose=False,\n            val_verbose=True,\n            summary_verbose=False,\n            validating_frequency=config.checkpointing\n            )\n\n        model_name, record = pick_best_model(\n            path.join(config.title, wavelength),\n            config.target_metric\n            )\n\n        trainer.model = torch.load(\n            path.join(config.title, wavelength, \"checkpoints\", model_name),\n            weights_only=False, map_location=config.device)[\"model\"]\n        trainer.threshold = record[\"threshold\"]\n\n        trainer.test(verbose=True)\n\n        if config.PURNE_CHECKPOINTS:\n            ckpts_dir = path.join(config.title, wavelength, \"checkpoints\")\n            for ckpt in os.listdir(ckpts_dir):\n                if ckpt != model_name:\n                    os.remove(path.join(ckpts_dir, ckpt))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T18:13:17.077743Z","iopub.execute_input":"2025-11-27T18:13:17.078050Z","iopub.status.idle":"2025-11-27T18:13:17.101421Z","shell.execute_reply.started":"2025-11-27T18:13:17.078033Z","shell.execute_reply":"2025-11-27T18:13:17.100417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Testing each classifier block is -commented for records-\n\n# for wavelength in config.wavelengths:\n#     print(f\"current wavelength: {wavelength}\")\n\n#     real_dataset = WaveLenghtDatasetV2(\n#         config.train_data_dir,\n#         train_meta,\n#         wavelength,\n#         preprocessor,\n#         augmentation\n#         )\n\n#     if config.synthesized_sample_size is not None \\\n#         and config.synthesized_images_dir is not None:\n#         synthesized_dataset = SynthesizedDataset(\n#             config.synthesized_images_dir,\n#             wavelength,\n#             synthesized_class=1,\n#             transform=preprocessor,\n#             augmentation=augmentation,\n#             num_samples=config.synthesized_sample_size\n#         )\n#         train_dataset = MergedDatasets(real_dataset, synthesized_dataset)\n#         weights = torch.cat([\n#             torch.tensor(list(real_dataset._metadata[\"peak_flux\"].iloc())).int(),\n#             torch.ones(len(synthesized_dataset))\n#             ]).int()\n#     else:\n#         train_dataset = real_dataset\n#         weights = torch.tensor(list(real_dataset._metadata[\"peak_flux\"].iloc())).int()\n\n#     val_dataset = WaveLenghtDatasetV2(\n#         config.train_data_dir,\n#         val_meta,\n#         wavelength,\n#         preprocessor\n#         )\n\n#     test_dataset = WaveLenghtDatasetV2(\n#         config.test_data_dir,\n#         test_meta,\n#         wavelength,\n#         preprocessor\n#     )\n\n#     sampler = WeightedRandomSampler(\n#         weights=(weights.bincount() * (torch.tensor(config.oversampling_ratio) if config.oversampling_ratio is not None else 1))[weights],\n#         num_samples=len(weights),\n#         replacement=True\n#     )\n\n#     os.makedirs(config.cache_dir, exist_ok=True)\n\n#     sub_title = path.join(config.title, wavelength)\n\n#     model_name, record = pick_best_model(\n#         path.join(config.title, wavelength),\n#         config.target_metric\n#         )\n\n#     model = torch.load(\n#         path.join(config.title, wavelength, \"checkpoints\", model_name),\n#         weights_only=False, map_location=config.device)[\"model\"]\n\n#     optim = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n#     train_loader = DataLoader(train_dataset, batch_size=config.batch_size, sampler=sampler)\n#     val_loader = DataLoader(val_dataset, batch_size=config.batch_size)\n#     test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n\n#     trainer = Trainer(\n#             sub_title,\n#             model,\n#             optim,\n#             loss_fn,\n#             eval_metrics,\n#             train_loader,\n#             val_loader,\n#             test_loader,\n#             checkpointing=config.checkpointing,\n#             accumulate_gradient=max(1, int(config.batch_size / config.effective_batch_size)),\n#             threshold=record[\"threshold\"],\n#             dynamic_thresholding=config.dynamic_thresholding,\n#             dynamic_thresholding_metric=config.target_metric,\n#             progress_bar_update=config.progress_bar_update,\n#             device=config.device\n#             )\n\n#     trainer.test(verbose=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T18:13:17.102428Z","iopub.execute_input":"2025-11-27T18:13:17.102942Z","iopub.status.idle":"2025-11-27T18:13:17.119664Z","shell.execute_reply.started":"2025-11-27T18:13:17.102901Z","shell.execute_reply":"2025-11-27T18:13:17.118899Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# training ML classifier to try a different classification head\n\nif config.TRAIN_CLASSIFIER:\n    train_dataset = CacheDataset(\n        path.join(config.cache_dir, \"train\"),\n        SDOCacheTransform()\n    )\n\n    val_dataset = CacheDataset(\n        path.join(config.cache_dir, \"val\"),\n        SDOCacheTransform()\n    )\n\n    test_dataset = CacheDataset(\n        path.join(config.cache_dir, \"test\"),\n        SDOCacheTransform()\n    )\n\n    torch.save(\n        ClassifierTrainer.tensorize(None, train_dataset),\n        path.join(config.cache_dir, \"classifier\", \"train.pt\")\n        )\n\n    torch.save(\n        ClassifierTrainer.tensorize(None, val_dataset),\n        path.join(config.cache_dir, \"classifier\", \"val.pt\")\n        )\n\n    torch.save(\n        ClassifierTrainer.tensorize(None, test_dataset),\n        path.join(config.cache_dir, \"classifier\", \"test.pt\")\n        )\n\n    model_pick = Classifier.list_models() if config.classifier == \"try_all\" else [config.classifier]\n    for model_name in model_pick:\n        print(f\"current model: {model_name}\")\n        classifer_trainer = ClassifierTrainer(\n            path.join(config.title, model_name),\n            Classifier(use_smote=config.use_smote, random_state=config.seed),\n            eval_metrics=eval_metrics,\n            train_dataset=path.join(config.cache_dir, \"classifier\", \"train.pt\"),\n            val_dataset=path.join(config.cache_dir, \"classifier\", \"val.pt\"),\n            test_dataset=path.join(config.cache_dir, \"classifier\", \"test.pt\"),\n            dynamic_thresholding=config.dynamic_thresholding,\n            dynamic_thresholding_metric=config.target_metric\n        )\n\n        classifer_trainer.fit()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-27T18:46:14.368Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# comparing models performance on the testing dataset\nrecords = []\nfor wavelength in config.wavelengths:\n    record = read_json(path.join(config.title, wavelength, \"test_records\", \"fixed_record.json\"))\n    records.append(record)\n\nplot_metric_bars(records, config.wavelengths, cols=2, save_dir=path.join(config.title, \"models_test_comparison_plot.png\"), metric=config.target_metric)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T18:13:17.120429Z","iopub.execute_input":"2025-11-27T18:13:17.120626Z","iopub.status.idle":"2025-11-27T18:13:21.249780Z","shell.execute_reply.started":"2025-11-27T18:13:17.120611Z","shell.execute_reply":"2025-11-27T18:13:21.248971Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encoding the datasets for quicker training\nif config.ENCODE:\n    models = {}\n    records = []\n    for wavelength in config.wavelengths:\n        model_name, record = pick_best_model(\n            path.join(config.title, wavelength),\n            config.target_metric\n            )\n        records.append(record)\n\n        model = torch.load(path.join(config.title, wavelength, \"checkpoints\", model_name), weights_only=False, map_location=config.device)[\"model\"]\n        models[wavelength] = model.eval()\n\n    plot_metric_bars(records, config.wavelengths, cols=2, save_dir=path.join(config.title, \"models_validation_comparison_plot.png\"), metric=config.target_metric)\n\n    sdo_preprocessor = T.Compose([\n        ReadImgs(\"stack\"),\n        T.Lambda(lambda x: x.unsqueeze(1)),\n        T.Resize((config.image_size, config.image_size)),\n        T.ConvertImageDtype(dtype=torch.float32),\n        FrequencyChannelTransform(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n    train_dataset = SDODataset(\n        config.train_data_dir,\n        train_meta,\n        config.wavelengths,\n        sdo_preprocessor\n    )\n\n    val_dataset = SDODataset(\n        config.train_data_dir,\n        val_meta,\n        config.wavelengths,\n        sdo_preprocessor\n    )\n\n    test_dataset = SDODataset(\n        config.test_data_dir,\n        test_meta,\n        config.wavelengths,\n        sdo_preprocessor\n    )\n\n    os.makedirs(path.join(config.cache_dir, \"train\"), exist_ok=True)\n    os.makedirs(path.join(config.cache_dir, \"val\"), exist_ok=True)\n    os.makedirs(path.join(config.cache_dir, \"test\"), exist_ok=True)\n\n    os.makedirs(path.join(config.cache_dir, \"train\", \"1\"), exist_ok=True)\n    os.makedirs(path.join(config.cache_dir, \"train\", \"0\"), exist_ok=True)\n    os.makedirs(path.join(config.cache_dir, \"val\", \"1\"), exist_ok=True)\n    os.makedirs(path.join(config.cache_dir, \"val\", \"0\"), exist_ok=True)\n    os.makedirs(path.join(config.cache_dir, \"test\", \"1\"), exist_ok=True)\n    os.makedirs(path.join(config.cache_dir, \"test\", \"0\"), exist_ok=True)\n\n    with torch.no_grad():\n        id_counter = 0\n        for i, (imgs_dict, label) in enumerate(tqdm(train_dataset, desc=\"encoding...\")):\n            encodings = {}\n            for wavelength, img in imgs_dict.items():\n                out = models[wavelength].features(img.to(config.device))\n                encodings[wavelength] = models[wavelength].avgpool(out).flatten(1)\n\n            for timestep in range(4):\n                timestep_dict = {wavelength : encodings[wavelength][timestep] for wavelength in encodings.keys() if encodings[wavelength][timestep].size(0) > timestep}\n                save_dir = path.join(config.cache_dir, \"train\", str(int(label.item())), f\"{id_counter}_{i}_{timestep}.pt\")\n                id_counter += 1\n                torch.save(timestep_dict, save_dir)\n\n        id_counter = 0\n        for i, (imgs_dict, label) in enumerate(tqdm(val_dataset, desc=\"encoding...\")):\n            encodings = {}\n            for wavelength, img in imgs_dict.items():\n                out = models[wavelength].features(img.to(config.device))\n                encodings[wavelength] = models[wavelength].avgpool(out).flatten(1)\n\n            for timestep in range(4):\n                timestep_dict = {wavelength : encodings[wavelength][timestep] for wavelength in encodings.keys() if encodings[wavelength][timestep].size(0) > timestep}\n                save_dir = path.join(config.cache_dir, \"val\", str(int(label.item())), f\"{id_counter}_{i}_{timestep}.pt\")\n                id_counter += 1\n                torch.save(timestep_dict, save_dir)\n\n        id_counter = 0\n        for i, (imgs_dict, label) in enumerate(tqdm(test_dataset, desc=\"encoding...\")):\n            encodings = {}\n            for wavelength, img in imgs_dict.items():\n                out = models[wavelength].features(img.to(config.device))\n                encodings[wavelength] = models[wavelength].avgpool(out).flatten(1)\n\n            for timestep in range(4):\n                timestep_dict = {wavelength : encodings[wavelength][timestep] for wavelength in encodings.keys() if encodings[wavelength][timestep].size(0) > timestep}\n                save_dir = path.join(config.cache_dir, \"test\", str(int(label.item())), f\"{id_counter}_{i}_{timestep}.pt\")\n                id_counter += 1\n                torch.save(timestep_dict, save_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T18:13:21.250542Z","iopub.execute_input":"2025-11-27T18:13:21.250765Z","iopub.status.idle":"2025-11-27T18:13:21.265234Z","shell.execute_reply.started":"2025-11-27T18:13:21.250744Z","shell.execute_reply":"2025-11-27T18:13:21.264561Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training Fusion model (the model that trains based on the embedding of all the single-channel models)\nif config.TRAIN_FUSION:\n    fusion_models = [\n        (\"resnet_1D\", lambda : WideResNet1D(in_channels=len(config.wavelengths), num_channels=[1024, 512, 256, 128]))\n        # (\"robert\", lambda : RobertaSeqClassifier(embed_dim=config.embed_dim))\n        # (\"distil_bert\", lambda : DistilBertSeqClassifier(embed_dim=config.embed_dim)),\n    ]\n\n    for sub_title, model in fusion_models:\n        train_dataset = CacheDataset(\n            path.join(config.cache_dir, \"train\"),\n            SDOCacheTransform()\n        )\n\n        val_dataset = CacheDataset(\n            path.join(config.cache_dir, \"val\"),\n            SDOCacheTransform()\n        )\n\n        test_dataset = CacheDataset(\n            path.join(config.cache_dir, \"test\"),\n            SDOCacheTransform()\n        )\n\n\n        labels = train_dataset.get_labels()\n        freq_count = [len(labels) - sum(labels), sum(labels)]\n\n        weights = (\n            freq_count\n            if config.oversampling_ratio is None\n            else [w * f for w, f in zip(config.oversampling_ratio, freq_count)]\n            )\n\n        sampler = WeightedRandomSampler(\n            weights=[weights[label] for label in labels],\n            num_samples=len(labels),\n            replacement=True\n        )\n\n        sub_title = path.join(config.title, sub_title)\n        model = model()\n        optim = torch.optim.AdamW(model.parameters(), lr=1)\n        scheduler = torch.optim.lr_scheduler.LinearLR(optim, start_factor=config.fusion_learning_rate, end_factor=1e-5, total_iters=30)\n        loss_fn = HybridLossFunction(\n            FocalLoss(weights=freq_count, gamma=3),\n            LogitAdjustedBCE(weights=freq_count, tau=1.0),\n            weights=[0.7, 0.3]\n        )\n\n        trainer = Trainer(\n            sub_title,\n            model,\n            optim,\n            loss_fn,\n            eval_metrics,\n            DataLoader(train_dataset, batch_size=config.fusion_batch_size, sampler=sampler),\n            DataLoader(val_dataset, batch_size=config.fusion_batch_size),\n            DataLoader(test_dataset, batch_size=config.fusion_batch_size),\n            checkpointing=config.checkpointing,\n            lr_scheduler=scheduler,\n            lr_step_frequency=int(len(train_dataset) / 3),\n            accumulate_gradient=max(1, int(config.fusion_batch_size / config.effective_batch_size)),\n            threshold=config.threshold,\n            dynamic_thresholding=config.dynamic_thresholding,\n            dynamic_thresholding_metric=config.target_metric,\n            progress_bar_update=config.progress_bar_update,\n            device=config.device\n        )\n\n        trainer.fit(\n            config.fusion_num_epochs,\n            validating_frequency=config.checkpointing\n            )\n\n        trainer.test(verbose=True)","metadata":{"jupyter":{"source_hidden":true},"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T18:13:21.265857Z","iopub.execute_input":"2025-11-27T18:13:21.266159Z","execution_failed":"2025-11-27T18:46:14.368Z"}},"outputs":[],"execution_count":null}]}